# (PART) Análisis de Regresión {-}

# Regresión lineal {#cap-lm-applied}

## Resumen

En este instructivo, aprendemos a estimar modelos de regresión lineal en R usando la función básica `lm()`, interpretar y presentar los resultados y realizar diagnósticos y modificaciones a los modelos que estimamos. El objetivo final es utilizar técnicas estadísticas para evaluar nuestros modelos teóricos.

- Principales conceptos: regresión lineal, tabla de regresión, predicciones (valores esperados), diagnósticos.
- Principales funciones: `lm()`, `modelsummary()`, `ggpredict()`.

Vamos a utilizar las siguientes librerías:

```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(lmtest) # diagnosticos modelos
library(sandwich) # corregir errores estandar
library(ggeffects) # efectos en modelos de regresion
library(modelsummary) # tablas de regresion
```


```{r plot-theme, message=FALSE, warning=FALSE}
theme_set(theme_classic(base_size = 12))
```

## Modelos de regresión lineal en R

Supongamos que tenemos una pregunta sobre la relación (causal) entre el ingreso de un individuo y su nivel educativo. Nuestra hipótesis es que a mayor número de años de educación, mayor ingreso anual en promedio. Buscamos y encontramos datos al respecto (y los guardamos en el archivo `riverside_final.csv`, disponible [aquí](https://github.com/josefortou/lab-book/blob/master/data/p4v2017.xls)) y los cargamos en R: 

```{r cargar-datos}
# cargar datos
riverside <- read_csv("data/riverside_final.csv")
```

Reorganizamos un poco los datos -recodificamos unas variables- y miramos las primeras filas:

```{r ver-datos}
# recodificar variables categóricas
riverside <- riverside %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Mujer", "Hombre")),
    party = factor(party, levels = c(0, 1, 2), labels = c("Rep.", "Dem.", "Ind."))
  )

# imprimir los datos a la consola
head(riverside)
```

Vemos que tenemos información sobre ingreso, nivel educativo, experiencia laboral, género y afiliación partidista para `r tally(riverside)` individuos. Podemos empezar a trabajar.

## Análisis exploratorio y visualización

Empezamos haciendo un poco de análisis exploratorio de datos. Primero, veamos un resumen de las variables usando la función `summary()`:

```{r summary-data}
summary(riverside)
```

Usemos la función `datasummary_skim()` de la librería `modelsummary` para ver un resumen similar, pero con información adicional:

```{r skim}
datasummary_skim(riverside)
```

Si tenemos variables categóricas:

```{r skim-cat}
datasummary_skim(
  riverside, type = "categorical",
)
```

Y una tabla de correlaciones entre las variables numéricas:

```{r cor-table}
datasummary_correlation(riverside)
```

Como nuestras principales variables de interés (ingreso y educación) ambas son numéricas, un gráfico de dispersión de ambas es apropiado:

```{r scatter1}
riverside %>%
  ggplot(aes(edu, income)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Educación (años)", y = "Ingreso anual (USD)")
```

Parece que hay una relación lineal positiva entre ambas variables. Sin embargo, después de revisar la literatura, creemos que la experiencia laboral y el género de un individuo también impactan su ingreso. Así que exploramos la relación entre la experiencia profesional (en años) de cada individuo y su ingreso:

```{r scatter2}
riverside %>%
  ggplot(aes(senior, income)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Experiencia laboral (años)", y = "Ingreso anual (USD)")
```

Vemos un patrón similar. Además, podemos mirar la relación entre género -una variable categórica binaria en nuestros datos- e ingreso:

```{r boxplot}
riverside %>%
  ggplot(aes(gender, income)) +
  geom_boxplot() + # caja y bigote
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Género", y = "Ingreso anual (USD)")
```

## Análisis de regresión

Basados en nuestra teoría y la literatura, creemos que el ingreso de un individuo $i$ es una función lineal del nivel de educación, la experiencia laboral y el género de dicho individuo. Entonces, el modelo que vamos a estimar es el siguiente:

$$ \hat{\text{Ingreso}}_i = \hat{\alpha} + \hat{\beta}_1 \text{Educación}_i + \hat{\beta}_2 \text{Experiencia}_i + \hat{\beta}_3 \text{Hombre}_i + \hat{u}_i $$

### Estimación

En R, estimamos modelos de regresión lineal usando la función `lm()`. Esta función toma como principal argumento una fórmula de la forma `y ~ x + z`, donde `y` es la variable dependiente y `x` y `z` son variables independientes.

Estimemos un modelo simple ($\text{Ingreso}_i = \alpha + \hat{\beta} \text{Educación}_i$) y guardémoslo como un nuevo objeto:

```{r mod-simple}
modelo_simple <- lm(income ~ edu, data = riverside)
```

Así de sencillo. Si queremos explorar los resultados del modelo, podemos usar la función `summary()`:

```{r}
summary(modelo_simple)
```

Vemos varios elementos, incluyendo coeficientes (`Estimate`), errores estándar (`Std. Error`) y *p-values* (`Pr(>|t|)`) de cada variable, más unas estadísticas del ajuste del modelo en general (como el $R^2$). Estos elementos se pueden extraer individualmente, por ejemplo:

```{r}
coef(modelo_simple)
```

Por ahora, nos vamos a enfocar en los coeficientes. Recordemos brevemente la interpretación básica de los coeficientes de un modelo de regresión lineal: 

- El intercepto ($\alpha$) es el valor esperado de $Y$ (ingreso, en este caso) cuando todas las variables independientes $= 0$.
- El coeficiente de regresión de una variable independiente ($\beta$) es el incremento en el valor esperado de $Y$ (ingreso) asociado a un incremento de *una unidad* en $X$ (educación).

Podemos tener modelos de regresión con variables independientes categóricas, por ejemplo binarias (también llamadas variables *dummy* o variables indicador). Usemos la variable `gender` que toma dos valores en los datos: "Hombre" y "Mujer". Estimemos el modelo con la misma función, pero cambiando la variable independiente:

```{r mod-dummy}
modelo_genero <- lm(income ~ gender, data = riverside)
summary(modelo_genero)
```

Recordemos que cuando se trata de variables independientes binarias, la interpretación de los coeficientes cambia levemente:

- $\alpha$ es el valor esperado de $Y$ cuando el valor de $X$ es la categoría de referencia - en este caso, $\hat{Y}$ cuando $X = \text{Mujer}$. 
- $\beta$ es la diferencia en el valor esperado de $Y$ para la(s) otra(s) categoría(s) de la variable categórica.^[Una variable categórica con más de 2 categorías pueden conceptualizarse como una serie de variables *dummy*.] En este caso, cuando $X = 1$ (hombre), $\hat{Y} = \hat{\alpha} + \hat{\beta}$. Cuando $X = 0$ (mujer), entonces $\hat{Y} = \hat{\alpha}$.

#### Regresión múltiple

Sin embargo, dado nuestro modelo teórico, nos interesa estimar un modelo de regresión **múltiple** con más de una variable independiente. Para esto,  agregamos más variables independientes a la fórmula con el operador `+`. Estimamos el modelo, lo guardamos como un objeto e imprimimos a la consola un resumen:

```{r mod-completo}
modelo_ingreso <- lm(income ~ edu + senior + gender, data = riverside)
summary(modelo_ingreso)
```

Esta información es suficiente para interpretar la magnitud, dirección y significancia de los coeficientes. Luego volveremos a este modelo para discutir cómo interpretarlo.

#### Especificación del modelo

Además de añadir términos (variables independientes) a un modelo, podemos también cambiar la forma en que las variables entran al modelo. Dos transformaciones comunes son los logaritmos y los términos multiplicativos.

##### Logaritmos

La regresión lineal está diseñada para funcionar mejor con variables dependientes con distribuciones normales (o cercanas a la normal). Cuando encontramos una variable con una distribución sesgada hacia la derecha (muchos casos con valores bajos), aplicarle la función logarítmica permite mover la distribución de forma tal que se acerque más a la normal. Como la función logarítmica aplica una transformación afín, no cambia la estructura subyacente de los datos. Por otro lado, 

<!-- Por estas razones metodológicas y teóricas, las variables en escala logarítmica son comunes en modelos de regresión. La siguiente gráfica muestra lo que sucede cuando aplicamos la función logarítmica a una variable con una distribución sesgada hacia la derecha (en este caso, una variable con distribución Gamma): -->

<!-- ```{r distrib-log} -->
<!-- tibble( -->
<!--   variable_gamma = rgamma(10000, 3), -->
<!--   variable_log_normal = log(variable_gamma) -->
<!-- )  %>% -->
<!--   pivot_longer(variable_gamma:variable_log_normal,  -->
<!--                names_to = "distrib", names_prefix = "variable_", -->
<!--                values_to = "valores") %>% -->
<!--   ggplot(aes(valores)) + -->
<!--   geom_histogram()+ -->
<!--   facet_wrap(~ distrib, scales = "free", ncol = 1) + -->
<!--   labs(x = "Valores", y = "Número de observaciones") -->
<!-- ``` -->

Podríamos pensar que la asociación entre experiencia laboral e ingreso no es lineal, sino que exhibe rendimientos decrecientes: pasar de 0 a 5 años de experiencia se asociaría con un aumento en el ingreso mayor que pasar de 10 a 15 años de experiencia. Es posible que una transformación logarítmica de la variable `senior` nos permita capturar intuiciones teóricas como los rendimientos decrecientes. Recordemos la forma de una función logarítmica para hacer la conexión con esta idea teórica:

```{r}

tibble(
  x = 1:100,
  log_x = log(x)
) %>%
  ggplot(aes(x, log_x)) +
  geom_line(color = "red")
```

Para valores bajos de $x$, el aumento correspondiente de $\log(x)$ es mayor que el aumento cuando los valores de $x$ son mayores.

Ahora, exploremos la relación entre el logaritmo natural de la experiencia en años y el ingreso:

```{r scatter-log}
riverside %>%
  ggplot(aes(log(senior), income)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  scale_x_continuous(trans = "log") +
  labs(x = "Experiencia laboral (años), log.", y = "Ingreso anual (USD)")
```

Podemos estimar el logaritmo (natural) y usarlo directamente en un modelo en la fórmula de `lm()` con la función `log()`:

```{r mod-log}
lm(income ~ edu + log(senior) + gender, data = riverside) %>%
  summary()
```

Sin embargo, es preferible realizar la transformación antes, para poder tener información (como por ejemplo, estadísticas resumen) de la variable transformada, como lo hicimos arriba - aquí repetimos la operación como demostración:

```{r mutate-log}
riverside <- riverside %>%
  mutate(senior_log = log(senior))
```

Para estimar el modelo con la variable de experiencia en versión logaritmo, simplemente la agregamos a la fórmula en `lm()`:

```{r mod-log2}
modelo_log <- lm(income ~ edu + senior_log + gender, data = riverside)
summary(modelo_log)
```

Recordemos que en este caso, los demás coeficientes y el intercepto se interpretan igual, pero el coeficiente para la variable transformada se interpreta de manera diferente: un aumento de $1\%$ en $X$ (ojo, no en $\log(X)$) se asocia con un incremento de $\hat{\beta} \times 0.01$ en $\hat{Y}$. 

Es posible también transformar la variable dependiente de manera logarítmica (un modelo log-normal) o tanto la dependiente como una independiente (un modelo log-log). La interpretación en esos casos también cambia, pero no la discutimos aquí.

##### Términos multiplicativos o interacciones

Supongamos que creemos que la asociación entre $X_1$ y $Y$ (el coeficiente $\beta_1$) depende del valor de $X_2$.^[O, a la inversa, que la asociación entre $X_2$ y $Y$ (el coeficiente $\beta_2$) depende del valor de $X_1$.] En otras palabras, creemos que hay una variable que modera la asociación entre $X_1$ y $Y$. En este caso, podemos estimar un modelo lineal con términos multiplicativos o interacciones entre las variables independientes. Las interacciones nos permiten capturar una noción teórica interesante que nos permiten capturar esta idea. 

En términos prácticos, una interacción es una multiplicación entre dos variables. Pueden ser dos variables numéricas, dos categóricas (recordemos que para efectos de estimar modelos, las categorías se convierten en enteros) o una numérica y una categórica. La interacción más común es la interacción entre una variable numérica y una categórica. En este caso, podríamos pensar que la relación entre educación (una variable numérica) e ingreso es distinta para hombres y para mujeres (categorías). Incorporamos esta noción teórica al modelo que vamos a estimar:

$$ \hat{\text{Ingreso}}_i = \hat{\alpha} + \hat{\beta}_1 \text{Educación}_i + \hat{\beta}_2 \text{Hombre}_i + \hat{\beta}_3 (\text{Educación}_i \times \text{Hombre}_i) + \hat{u}_i $$

En R, simplemente multiplicamos los dos términos en la fórmula de `lm()` - esto se puede hacer antes y crear una nueva columna en los datos:

```{r mod-inter}
modelo_inter <- lm(income ~ edu*gender, data = riverside)
summary(modelo_inter)
```

Noten que el modelo incluye tanto el término multiplicativo (`edu:genderHombre`), como los componentes del mismo; esto es por diseño. La interpretación de los coeficientes de las variables que no hacen parte de una interacción permanece igual, pero la forma en que interpretamos la asociación entre $X_1$ y $Y$ y entre $X_2$ y $Y$ cambia de manera importante:

- Cuando $X_2 = 0 = \text{"Mujer"}$, el intercepto es $\alpha$ y la asociación entre $X_1$ y $Y$ es $\beta_1$.
- Cuando $X_2 = 1 = \text{"Hombre"}$, el intercepto es $\alpha + \beta_2$ y la asociación entre $X_1$ y $Y$ es $\beta_1 + \beta_3$.

### Diagnósticos

<!-- Hagamos unos diagnósticos básicos del ajuste y los supuestos de nuestro modelo de regresión lineal. Empecemos por extraer algunos elementos del modelo estimado y organizándolos en un `tibble`: -->

<!-- ```{r augment-mod} -->
<!-- # datos (observaciones y variables) usados en el modelo -->
<!-- modelo_aug <- as_tibble(modelo_ingreso$model) -->
<!-- modelo_aug <- modelo_aug %>% -->
<!--   mutate(fitted = modelo_ingreso$fitted.values, -->
<!--          residuals = modelo_ingreso$residuals, -->
<!--          std_residuals = rstandard(modelo_ingreso), -->
<!--          cooks_d = cooks.distance(modelo_ingreso)) -->
<!-- modelo_aug -->
<!-- ``` -->


<!-- Para entender un poco mejor si nuestro modelo captura los patrones subyacentes en los datos, podemos mirar una gráfica de dispersión de las variables independientes numéricas versus los residuos. En este caso, como no vemos ningún patrón evidente, creemos que el modelo captura señal y no solo ruido: -->

<!-- ```{r resid-x} -->
<!-- modelo_aug %>% -->
<!--   ggplot(aes(residuals, edu)) +  -->
<!--   geom_point() + -->
<!--   labs(x = "Educación (años)", y = "Residuos.") -->
<!-- modelo_aug %>% -->
<!--   ggplot(aes(residuals, senior)) +  -->
<!--   geom_point() + -->
<!--   labs(x = "Experiencia (años)", y = "Residuos") -->
<!-- ``` -->

El modelo de regresión lineal por mínimos cuadrados ordinarios (MCO u OLS) es el estimador lineal insesgado de mínima varianza ("OLS is BLUE") cuando se cumplen una serie de supuestos, entre ellos, supuestos sobre los errores $u$ y su distribución. R nos permite realizar unos diagnósticos para evaluar si estos supuestos se cumplen. 

#### Linealidadad

En OLS, asumimos que $Y$ es una función lineal de las variables independientes. Evaluamos este supuesto visualmente con una gráfica de los valores esperados del modelo (las predicciones) vs. los residuos del mismo, buscando si hay una relación entre ambos:

```{r linear-plot}
plot(modelo_ingreso, which = 1)
```

Si vemos una relación clara (¡miren la línea roja!), hay un problema de linealidad. Violar este supuesto es un problema de específicación del modelo, o sea qué variables incluimos y de qué forma entran. En este caso, podríamos transformar nuestras variables independientes  con logaritmos o funciones exponenciales.

#### Normalidad

OLS asume que los residuos (errores) del modelo están distribuidos normalmente. Nuevamente, realizamos una evaluación visual del supuesto (una gráfica Q-Q), buscando que los puntos no se desvíen mucho de la línea diagonal, especialmente en los extremos:

```{r}
plot(modelo_ingreso, which = 2)
```

Otra opción es usar un prueba estadística, como el test de Shapiro o el Kolmogorov-Smirnoff, vía las funciones `shapiro.test()`. Si obtenemos un *p-value* menor a 0.05, rechazamos la hipótesis nula de normalidad en la distribución de los residuos:

```{r}
shapiro.test(resid(modelo_ingreso))
```

Cuando vemos violaciones muy severas de este supuesto, puede que haya un problema con nuestra variable dependiente. Quizás en realidad sea una medida binaria, categórica u ordinal con pocos niveles. En estos casos, podemos buscar medidas alternativas u técnicas distintas a la regresión lineal por OLS.

#### Independencia

Un supuesto muy importante es que las observaciones son sacadas de una muestra de manera independiente; en otras palabras, que no hay patrones de dependencia espacial, temporal o multinivel entre observaciones. Este es un supuesto que podemos evaluar simplemente conociendo nuestros datos y teoría. ¿Mis casos son países que observo año tras año? ¿O quizás espero que lo que sucede en un municipio afecte a los vecinos? Esas son situaciones de autocorrelación temporal y/o espacial. Otra forma de evaluar este supuesto es viendo mapas o series de tiempo de nuestra variable dependiente en búsqueda de tendencias.

Sin embargo, también existen pruebas estadísticas para evaluar el supuesto de independencia. El test de Durbin-Watson, implementado con la función `dwtest()` de la librería `lmtest`, es una de estas pruebas:

```{r}
dwtest(modelo_ingreso)
```

Un *p-value* por encima de 0.05 es consistente con la hipótesis nula de no autocorrelación. 

Violaciones al supuesto de independencia se solucionan estimando un modelo distinto; en algunas ocasiones, *muy* distinto. Estos son temas avanzados que no cubrimos aquí, pero más adelante discutimos los estimadores de efectos fijos para analizar datos en panel.

#### Homoesquedasticidad

El supuesto de homoesquedasticidad significa que la varianza de los residuos es constante. Violar este supuesto -los errores están correlacionados entre sí, por ejemplo, y la varianza de los mismos no es constante- implica que los errores estándar estarían mal estimados (más pequeños de lo que deberían ser), dándonos una falsa sensación de confianza.

Para evaluar este supuesto, podemos construir un gráfico de valores esperados de $Y$ versus los residuos estandarizados (en realidad, la raíz cuadrada de los valores absolutos de estos errores). Cuando se cumple el supuesto, no debemos observar ninguna relación clara entre ambos valores:

```{r hetero-plot}
plot(modelo_ingreso, which = 3)
```

Buscamos que a) la línea roja sea aproximadamente horizontal y b) la dispersión de los puntos no cambie mucho en función de los valores esperados. Aquí parece que tenemos un problema de heteroesquedasticidad.

Un test de Breusch-Pagan nos permitiría ponerle más precisión a este diagnóstico. Usamos la función `bptest()`, nuevamente de la librería `lmtest`. Veamos:

```{r}
bptest(modelo_ingreso)
```

La hipótesis nula en esta prueba es homoesquedasticidad. Si la rechazamos con un *p-value* bajo (menor a 0.05), tenemos heteroesquedasticidad. 

<!-- El test evalúa si hay homoesquedasticidad. En este caso, el *p-value* por encima de 0.05 significa que no rechazamos la hipótesis nula de homoesquedasticidad: -->

<!-- ```{r bp-test} -->
<!-- bptest(modelo_ingreso) -->
<!-- ``` -->

<!-- Si en cambio $p < 0.05$, rechazamos la hipótesis nula de homoesquedasticidad e inferimos que hay heteroesquedasticidad en nuestro modelo. Esto implica que los nuestros errores estándar del modelo pueden ser menores de lo que deberían ser. Existen ajustes para este problema, como usar "Weighted Least Squares" (WLS) o transformar las variables independientes. -->

Una de las razones por las que podemos encontrar heteroesquedasticidad en un modelo es que tenemos observaciones con mucha influencia sobre los resultados; podríamos decir que son atípicas. La $D$ de Cook es una estadística que nos permite medir la influencia de una observación en un modelo de regresión lineal. Vamos a calcular esta estadística para cada una de las observaciones que utilizamos en nuestro modelo y vamos a colocarlas junto a dichas observaciones en un `tibble`:

```{r cook-d}
datos_modelo <- bind_cols(
  modelo_ingreso$model,
  d_cook = cooks.distance(modelo_ingreso)
)
head(datos_modelo)
```

Ahora, podemos buscar y eliminar las observaciones con una $D$ de Cook alta. Dos reglas informales sugieren que observaciones con valores por encima de 1 o con valores por encima de 3 veces la media pueden tener alta influencia. Vamos a eliminar estas observaciones, reestimar el modelo y volver a evaluar el supuesto de homoesquedasticidad:

```{r remove-outliers}
datos_modelo %>%
  filter(d_cook < 3*mean(d_cook, na.rm = TRUE)) %>%
  lm(income ~ edu + senior + gender, data = .) %>%
  plot(., which = 3)
```

Parece que no mejoró mucho la situación. Otra opción para lidiar con los problemas asociados con la heteroesquedasticidad es corregir los errores estándar después de estimar el modelo y diagnosticarlo. Después de todo, sabemos que ese es el efecto de violar el supuesto. Vamos a utilizar la función `coeftest()` de la librería `sandwich` para recalcular los errores estándar. El resultado son lo que llamamos "erorres estándar robustos":

```{r}
coeftest(modelo_ingreso, vcov = vcovHC)
```

Noten que los errores estándar de las variables `edu` y `senior` ahora son más grandes. La práctica de estimar errores estándar robustos de distintos tipos es bastante común en en Ciencia Política y Economía.

Hay otras soluciones al problema de heteroesquedasticidad, como aplicar "weighted least squares" (WLS) o usar técnicas como el *bootstrap*. Por el momento, procedemos con nuestro análisis y reportamos nuestros resultados, pero entendiendo que nuestros errores estándar pueden estar mal estimados. 

## Más Interpretación

Hasta ahora, hemos discutido brevemente cómo interpretar los coeficientes de un modelo de regresión múltiple con distintas especificaciones, utilizando la función `summary()` para ver estos resultados. Pero podemos refinar un poco más la interpretación. 

<!-- La librería `broom` ofrece varias funciones para trabajar con modelos estadísticos. Primero, la función `tidy()` nos permite ver los coeficientes y su significancia en forma de `tibble`: -->

<!-- ```{r mod-tidy} -->
<!-- tidy(modelo_ingreso) -->
<!-- ``` -->

```{r}
summary(modelo_ingreso)
```

Podemos interpretar la **dirección, magnitud y significancia** de cada uno de los coeficientes utilizando lo que ya sabemos y nuestras discusiones previas:

- Educación (variable continua): *ceteris paribus*, cada incremento de una unidad en la variable se asocia con un incremento de aproximadamente `r coef(modelo_ingreso)[[2]] %>% round()` unidades en la variable dependiente. En otras palabras, manteniendo las demás variables constantes, un año adicional de educación se asocia en promedio con un incremento de `r coef(modelo_ingreso)[[2]] %>% round()` USD en el ingreso anual.
- Experiencia (continua): *ceteris paribus*, cada incremento de de una unidad en la variable se asocia con un incremento de aproximadamente `r coef(modelo_ingreso)[[3]] %>% round() * 0.01` unidades en la variable dependiente. En otras palabras, manteniendo las demás variables constantes, un año adicional de experiencia laboral se asocia en promedio con un incremento de `r coef(modelo_ingreso)[[3]] %>% round() * 0.01` USD en el ingreso anual.
- Género (categórica): *ceteris paribus*, ser hombre se asocia con un incremento de `r coef(modelo_ingreso)[[4]] %>% round()` unidades en la variable dependiente, comparado con la categoría de base (ser mujer). En otras palabras, manteniendo las demás variables constantes, el ingreso anual de un hombre es en promedio `r coef(modelo_ingreso)[[4]] %>% round()` USD que el de una mujer.

Estos tres coeficientes son estadísticamente significativos ($p < 0.05$), lo cual indica que la probabilidad de observar estos valores si la hipótesis nula ($\beta = 0$) fuese cierta es menos del 5%. O sea, es tan poco probable observar estos coeficientes, que ya que los observamos, debe ser que la hipótesis nula no representa el mundo real. En el mundo en que la hipótesis nula es cierta, $\beta = 0$ -- el *p-value* nos dice si el $\beta$ que observamos/estimamos encaja en un mundo en que $\beta = 0$.

<!-- Por otro lado, la función `summary()` también nos ofrece una mirada al ajuste del modelo en general. Entre otras estadísticas de "bondad del ajuste", el $R^2$ indica la proporción de la variación de $Y$ explicada por el modelo. A menos que estemos haciendo predicciones o comparando modelos similares que utilizan los mismos datos, no vale la pena dedicarle mucho tiempo al $R^2$: -->

<!-- ```{r mod-glance} -->
<!-- summary(modelo_ingreso) -->
<!-- ``` -->

<!-- Finalmente, la función `augment()` toma los datos utilizados en el modelo y los *aumenta* con los resultados del modelo, agregando el valor esperado de $Y$, el error estándar y los residuos $u$ para cada observación utilizada para estimar el modelo: -->

<!-- ```{r mod-augment} -->
<!-- modelo_aug <- augment(modelo_ingreso) -->
<!-- modelo_aug -->
<!-- ``` -->

<!-- Podemos utilizar este objeto para entender la diferencia entre el valor observado de la variable dependiente (`income`) y el valor esperado o la predicción del modelo (`.fitted`). Esas diferencias son los residuos. Por ejemplo, miremos la primera observación en la base de datos: -->

<!-- ```{r resid} -->
<!-- modelo_ingreso %>% -->
<!--   augment() %>% -->
<!--   slice(1) %>% # seleccionar la primera fila -->
<!--   select(edu, senior, gender, income, .fitted, .resid)  -->
<!-- ``` -->

<!-- Vemos que el modelo se equivoca (especificamente, predice menos ingreso del observado). Eso es normal. El modelo de regresión lineal estima la ecuación que mejor describe la asociación entre variables *en promedio*. -->

### Predicciones y efectos

Una de las herramientas más potentes que nos da un modelo de regresión lineal es la capacidad de hacer predicciones. Una predicción no es más que el valor esperado de $Y$ para determinados valores de las variables independientes. Hallamos estos valores esperados o predicciones utilizando los coeficientes estimados y reemplazando los valores de las variables independientes en la ecuación del modelo de regresión lineal. 

Calculemos un valor esperado "a mano" usando nuestro modelo. Primero, vamos a extraer los coeficientes del modelo usando `coef()` y `pluck()` - están en el orden en que aparecen en el resumen del modelo:

```{r}
alfa <- coef(modelo_ingreso) %>% pluck(1)
beta_edu <- coef(modelo_ingreso) %>% pluck(2)
beta_exp <- coef(modelo_ingreso) %>% pluck(3)
beta_gen <- coef(modelo_ingreso) %>% pluck(4)
```

Ahora, vamos a seleccionar valores de las variables independientes que nos interesan:

```{r}
intercepto <- 1 # siempre es 1
edu <- 10 # 10 años de educacion
exp <- max(riverside$senior, na.rm = TRUE) # ejecutiva
gen <- 0 # mujer
```

Finalmente, reemplazamos para hallar el valor esperado de $Y$:

```{r}
intercepto + beta_edu*edu + beta_exp+exp + beta_gen*gen
```

Comparemos con nuestra predicción para un hombre:

```{r}
gen <- 1
intercepto + beta_edu*edu + beta_exp+exp + beta_gen*gen
```

¡La diferencia entre ambos valores esperados es exactamente el coeficiente de la variable género!

Con esta información, podemos hacer inferencias sobre la relación entre las variables de interés. En otras palabras, podemos ver "efectos", asumiendo que hemos superado los distintos obstáculos a la inferencia causal.

Podemos automatizar, o si se quiere "tercerizar" este proceso. Digamos que estamos interesados en conocer el ingreso esperado de una mujer con educación y experiencia promedio en comparación con un hombre con las mismas cualificaciones. Vamos a usar la librería `ggeffects`, específicamente la función `ggpredict()`. Tomamos el modelo, lo pasamos a la función y especificamos las variables y los valores que nos interesan. La función mantiene las variables numéricas no especificadas constantes en sus valores promedio y mantiene las categóricas en su valor de referencia (es importante entonces recodificar bien):

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("gender"))
```

O de pronto nos interesa comparar el valor esperado del ingreso de individuos cuando se tiene poca y mucha experiencia:

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("senior[minmax]"))
```

Podemos incluir más variables:

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("gender","senior[minmax]"))
```


<!-- Por ejemplo, podríamos estar interesados en el ingreso esperado de una mujer con educación y experiencia promedio. Una forma de hacer esto es usando las funciones de `modelr`. Ponemos esta información en una tabla con `expand_grid()` y la pasamos a la función `add_predictions()`, especificando el modelo que queremos usar para hallar las predicciones: -->

<!-- ```{r pred} -->
<!-- riverside %>% -->
<!--   data_grid( -->
<!--     edu = mean(riverside$edu),  -->
<!--     senior = mean(riverside$senior), -->
<!--     gender = "Mujer" -->
<!--   ) %>% -->
<!--   add_predictions(modelo_ingreso)  -->
<!-- ``` -->

<!-- Otra forma de hacer esto de manera programática es si creamos una tabla que contenga combinaciones de las variables independientes y luego, con `add_predictions` agregamos los valores esperados de $Y$ (predicciones) para esas combinaciones. Aquí, por ejemplo, vamos de una unidad en una unidad por todo el rango de educación, mantenemos experiencia constante en la mediana e incluimos todos los valores de la variable binaria género: -->

<!-- ```{r pred2} -->
<!-- riverside %>% -->
<!--   data_grid( -->
<!--     edu = seq_range(edu, by = 1), # secuencia en el rango de la variable -->
<!--     senior = median(senior),  -->
<!--     gender  -->
<!--   ) %>% -->
<!--   add_predictions(modelo_ingreso) -->
<!-- ``` -->

<!-- Posteriormente, podemos filtrar el resultado para ver predicciones puntuales o usarlos para presentar resultados gráficamente (próxima sección). -->

## Presentar resultados

Dos formas principales de presentar los resultados de una regresión: tablas y gráficas. Vamos a utilizar las librerías `ggeffects` y `sjPlot` para esto.

### Tablas de regresión

Casi todos los artículos que utilizan modelos de regresión presentan los resultados en tablas. En ellas, típicamente vemos los coeficientes, intervalos de confianza o errores estándar, la significancia de los coeficientes (con *p-values*), el número de observaciones y algunas medidas del ajuste del modelo, como el $R^2$.

Para hacer tablas, podemos usar las librerías `stargazer`, `texreg`, `sjPlot` o `gtsummary`, entre otras, dependiendo de nuestras necesidades. Aquí, usamos `modelsummary()` de `modelsummary`. Le pasamos nuestro modelo y la función arroja una tabla formateada. 

```{r tab-simple}
modelsummary(modelo_ingreso)
```

Los argumentos de la función nos permiten cambiar muchos de los elementos de la tabla, en particular traducir algunos de inglés a español o usar intervalos de confianza en vez de errores estándar:

```{r tab-options}
modelsummary(
  modelo_ingreso,
  coef_map = c(
    "(Intercept)" = "(Intercepto)",
    "edu" = "Educación",
    "senior" = "Experiencia",
    "genderHombre" = "Género: hombre"
  ),
  statistic = "conf.int",
  output = "html",
  title = "Resultados del análisis."
)
```

Una tabla de regresión puede presentar varios modelos lado a lado para efectos de comparación. Si ponemos los modelos en una lista antes, podemos darles nombres en la tabla:

```{r tab-multi}
lista_modelos <- list(
  "Completo" = modelo_ingreso, 
  "Logaritmo" = modelo_log, 
  "Interacción" = modelo_inter
)
modelsummary(
  lista_modelos, # varios modelos
  coef_map = c(
    "(Intercept)" = "(Intercepto)",
    "edu" = "Educación",
    "senior" = "Experiencia",
    "senior_log" = "Experiencia (log.)",
    "genderHombre" = "Género: hombre",
    "edu:genderHombre" = "Educación × Género: hombre"
  )
)
```

Estas tablas las podemos exportar como archivos de tipo HTML, que después podemos incorporar a un documento Word o similar:

```{r tab-guardar, eval = FALSE}
modelsummary(
  modelo_ingreso,
  output = "output/tabla_modelo_ingreso.docx"
)
```

### Gráficas

Visualizar los resultados de modelos, especialmente las predicciones y la incertidumbre de estos, puede ayudar mucho a entenderlos mejor y comunicar nuestros resultados. Vamos a usar la función `ggpredict` de `ggeffects`. También podríamos usar `sjPlot` o hacerlo completamente "a mano".

<!-- Primero, veamos cómo visualizar los coeficientes y su significancia. Si aplicamos `plot_model()` al objeto que contiene el modelo, producimos una gráfica que nos muestra los coeficientes de cada variable y sus intervalos de confianza (al 95%). Como `plot_model()` usa `ggplot2`, podemos modificar y agergar elementos usando la misma gramática de las gráficas.^[También se pueden cambiar casi todos los elementos dentro de `plot_model()`.] Por ejemplo, agregamos una línea de referencia para mostrar si los intervalos cubren 0: -->

<!-- ```{r plot-coef} -->
<!-- modelo_ingreso %>% -->
<!--   plot_model() + -->
<!--   scale_x_discrete(labels = c("Género: hombre", "Experiencia", "Educación")) +  # los ejes están trocados -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs(title = "Resultados del modelo: coeficientes", -->
<!--        x = "Coeficientes", -->
<!--        y = "Estimados") -->
<!-- ``` -->

<!-- Así mismo, podemos comparar distintos modelos gráficamente con `plot_models()`: -->

<!-- ```{r plot-coef2} -->
<!-- plot_models( -->
<!--   modelo_ingreso, modelo_log, modelo_inter, -->
<!--   axis.labels = c("Educación", "Educación * hombre", "Hombre", "Experiencia",  -->
<!--                   "Experiencia (log)"), -->
<!--   legend.title = "Modelo", -->
<!--   m.labels = c("Completo", "Log", "Interacción") -->
<!-- ) -->
<!-- ``` -->

Como mencionamos arriba, podemos usar `ggpredict()` para ver los "efectos" de distintas variables, entendidos como el cambio en la variable dependiente para distintos valores de las independientes. Para esto, especificamos las variables independientes que nos interesan con `terms =`. Esta función mantiene constantes los factores (variables categóricas) en su categoría de referencia y las variables numéricas según su valor promedio. 

Le aplicamos la función al modelo y pasamos los resultados a `plot()`: aquí, vemos el valor esperado (predicción) de la variable dependiente ingreso en todo el rango de la variable educación, controlando por género (categoría de referencia: mujer) y experiencia (constante en la media). Como `ggeffects` funciona con `ggplot2`, podemos modificar y agregar utilizando los trucos que ya conocemos:

```{r plot-eff}
modelo_ingreso %>%
  ggpredict(terms = "edu") %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación",
       subtitle = "Mujeres, controlando por experiencia laboral",
       x = "Educación (años)", y = "Ingreso (USD)")
```

Adicionalmente, podemos especificar dos variables independientes para visualizar aún más variación. Vemos cómo la "línea base" de ingreso (el intercepto) es distinto para hombres y mujeres. Así, visualizamos más claramente la diferencia en ingreso para hombres y mujeres, por ejemplo. Además, esto evidencia que incluir una variable categórica esencialmente crea un intercepto distinto para cada grupo, sin cambiar la pendiente (el coeficiente): 

```{r plot-eff2}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "gender")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia", 
       x = "Educación (años)", y = "Ingreso (USD)", color = "Género")
```

Si las dos variables son numéricas, `ggpredict()` automáticamente escoge tres valores del rango de la segunda variable que le pasamos (incluyendo la media) y grafica tres líneas:

```{r plot-eff3}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "senior")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Educación (años)", y = "Ingreso (USD)", color = "Experiencia")
```

Podemos espeficiar cuántas líneas queremos ver y los valores que debe tomar la segunda variable independiente:

```{r plot-ef3}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "senior [5, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Educación (años)", y = "Ingreso (USD)", color = "Experiencia")
```

Finalmente, podemos cambiar el orden de las variables en `terms = ` si nos interesa resaltar el efecto de una variable más que la otra:

```{r plot-ef5}
modelo_ingreso %>%
  ggpredict(terms = c("gender", "edu [5, 15, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de género y educación",
       x = "Género", y = "Ingreso (USD)", color = "Educación")
```

```{r plot-ef6}
modelo_ingreso %>%
  ggpredict(terms = c("senior", "edu [5, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Experiencia (años, log)", y = "Ingreso (USD)", color = "Educación")
```

## Conclusiones

Utilidad de los modelos de regresión lineal:

- Predicción: si cumplimos con los supuestos básicos, podemos hacer predicciones puntuales precisas basados en la asociación entre variables.
- Explicación: si cumplimos con más supuestos (evitamos problemas de endogeneidad y de sesgo por variables omitidas), podemos hacer inferencias sobre relaciones de tipo causal.

Otras posibilidades:

- Variables dependientes categóricas: regresión logística y modelos por máxima verosimilitud (MLE o *maximum likelihood estimation*).
- Experimentos: podemos usar OLS para encontrar la diferencia de medias.
- Data Science: regresión como modelo predictivo que se puede "entrenar".

Utilizar R para el análisis estadístico:

- Curva de aprendizaje, pero grandes posibilidades.
- Practicar, practicar y practicar.
- Google es nuestro amigo.

## Taller: regresión lineal

Encontrar una base de datos relevante para el proyecto de investigación. Limpiar y ordenar los datos. El código de esta entrega **no** debe incluir todo el procedimiento para cargar la base de datos original y el proceso de limpieza y organización de la misma; solo el resultado final de este proceso.proceso.

### Estimación

Estimar dos modelos de regresión lineal **múltiple** usando la función `lm()` de R. Los modelos estimados deben cumplir con los siguientes requisitos (1.0 punto):

- La variable dependiente en ambos debe ser la variable dependiente del proyecto de investigación. *Idealmente* esta debe ser una variable de tipo continuo, pero excepcionalmente puede ser de tipo ordinal (si tiene $\geq 7$ categorías ordenadas) o binaria/dummy (entendiendo que la regresión lineal no es el mejor modelo para este tipo de variables). En estos dos casos, deben asegurarse que las variables estén codificadas como tipo numérico o `dbl` en R. 
- Las variables independientes deben ser a) factores potencialmente explicativos de la variable dependiente y b) variables de control. La selección de variables debe estar basada en su revisión de literatura, teoría e hipótesis.
- Debe haber por lo menos una variable independiente numérica en uno de los dos modelos. Si no tienen una, deben buscar más datos.
- Debe haber por lo menos una variable independiente categórica binaria en uno de los dos modelos. Si no tienen una, deben crearla a partir de otra.
- Dado el punto 5 de este taller, es recomendable que por lo menos uno de los modelos tenga *ambos* tipos de variables.
- *Sugerencia*: Si la literatura sugiere un debate entre dos enfoques teóricos, pueden estimar un modelo que represente cada enfoque (por ejemplo, un modelo solo con variables institucionales y otro solo con variables sociológicas), seguido de un modelo con todas las variables. 
- O pueden construir el modelo completo de manera progresiva: un modelo con solo variables independientes de interés teórico, seguido de uno que añade variables de control.

### Comunicación de resultados

Presentar los resultados de los modelos estimados en una sola tabla de regresión, usando las funciones de las librerías `texreg`, `sjPlot`, `modelsummary` o `stargazer` para producirla, teniendo en cuenta las siguientes indicaciones (1.0 punto). 

- La tabla debe tener título y los coeficientes deben estar nombrados apropiadamente (o sea, con algo distinto al nombre de la variable en la base de datos). Ambas funciones tienen estas opciones. 
- Deben guardar la tabla como archivo HTML.
- *Nota:* Pueden ejecutar `?nombre_de_fun` en la consola para explorar todas las opciones de estas funciones y cómo utilizarlas.

### Diagnóstico

Evaluar si los modelos estimados cumplen el supuesto de homoesquedasticidad ($\text{Var}(e_i) = \sigma^2$) y si hay observaciones con un grado alto de influencia. Esta evaluación puede hacerse de manera visual *o* gráfica o usando tests estadísticos. Deben incluir un breve párrafo en el que consignen sus apreciaciones al respecto (1.0 punto).
  
### Interpretación

Interpretar sustantivamente *todos* los coeficientes de los modelos incluídos en la tabla y la incertidumbre en torno a estos estimadores (pueden usar *p-values* o intervalos de confianza). *No* es necesario interpretar el coeficiente del intercepto. Cuando decimos "sustantivamente" nos referimos a que las interpretaciones deben hacer especial énfasis en la forma en que los resultados de los modelos se conectan con la evaluación de las hipótesis propuestas en el proyecto (1.0 punto).

### Interpretación gráfica

Usando uno de los dos modelos de regresión estimados en el primer punto, construir una gráfica con `ggpredict` donde se presenten valores predecidos o esperados de la variable dependiente (pronósticos) para el rango de valores de una variable numérica y para todas las categorías de una variable categórica binaria (manteniendo constantes las demás variables). La gráfica debe mostrar los valores esperados de la variable dependiente (con intervalos de confianza) para dos o más grupos (definidos por la variable categórica) a lo largo del rango de una variable independiente numérica (1.0 punto).

## Ejercicios adicionales

### Votos por Trump

Digamos que nos interesa estudiar los resultados de las elecciones presidenciales de 2016 en Estados Unidos. Para este proyecto, utilizamos una base de datos que incluye información sobre un conjunto de variables sociales y políticas a nivel de condado (municipio). Para esto, vamos a cargar una serie de librerías adicionales:

```{r}
library(socviz) # datos y herramientas para visualizar datos sociales
library(gtsummary) # tablas resúmen
library(texreg) # tablas de regresión
```

Veamos el resumen de algunas de estas variables - la tabla a continuación (construida con `tbl_summary()`) muestra la media y desviación estándar (variables numéricas) y el número de observaciones por grupo (variables categóricas):

```{r}
# seleccionamos y transformamos datos
county_data_sub <- county_data %>% 
  as_tibble() %>%
  select(name, state, census_region, black, white, hh_income,
         per_dem_2016, per_gop_2016, partywinner12) %>%
  mutate(partywinner12 = fct_explicit_na(partywinner12),
         black = black/100, 
         white = white/100, 
         hh_income = log(hh_income))

# estadisticas resumen
county_data_sub %>%
  select(-name, -state) %>%
  tbl_summary(
    missing = "no",
    label = list(black ~ "Prop. afro",
                 white ~ "Prop. blanco",
                 hh_income ~ "Ingreso promedio (log.)",
                 per_dem_2016 ~ "Prop. voto Dem.",
                 per_gop_2016 ~ "Prop. voto Rep.",
                 census_region ~ "Región",
                 partywinner12 ~ "Partido 2012"),
    statistic = list(all_continuous() ~ "{mean} ({sd})")
  ) %>%
  bold_labels()
```

Queremos entender la variación en la proporción de votos obtenidos por el candidato Demócrata (Donald Trump) en la elección de 2016 en cada condado. A continuación, vemos la distribución de esta variable:

```{r}
county_data %>%
  ggplot(aes(per_gop_2016)) +
  geom_histogram() +
  labs(x = "Prop. voto Rep.", y = "Número de condados")
```

Nuestra teoría sugiere que esperamos ver una mayor proporción de votos por Trump en condados con una mayor proporción de individuos que se identifican como blancos. La siguiente gráfica de dispersión explora esta relación:

```{r}
county_data %>%
  ggplot(aes(white, per_gop_2016)) +
  geom_point(alpha = 0.5) +
  labs(x = "Prop. blanco", y = "Prop. voto Rep.")
```

Para evaluar esta hipótesis, estimamos un modelo de regresión lineal múltiple. La variable dependiente es la proporción de votos por el candidato Republicano en cada condado. La variable independiente de interés es la proporción de habitantes blancos en cada condado. Además, incluimos varias variables de control: el logaritmo natural del ingreso promedio (en USD) de los hogares en cada condado, una *dummy* que indica el partido ganador en 2012 por condado y la región donde se ubica el condado. La ecuación que estimamos es:

$$ \hat{\text{Prop. voto Rep.}} = \alpha + \hat{\beta}_1 \times \text{Prop. blanco}_i + \hat{\beta}_2 \times \text{log(Ingreso)}_i + \hat{\beta}_3 \times \text{Partido 2012}_i + \hat{\beta}_4 \times \text{Región}_i + \hat{u}_i $$

La siguiente tabla muestra los resultados del modelo. Incluye el coeficiente ($\beta$) estimado para cada variable y el intercepto, así como intervalos de confianza del 95% entre paréntesis y la significancia estadística de cada coeficiente, además de unas estadísticas resumen del modelo (como el $\text{R}^2$).

```{r, results = "asis", eval = FALSE}
modelo_rep <- lm(per_gop_2016 ~ white + hh_income + partywinner12 + census_region, data = county_data)

htmlreg(
  modelo_rep, star.symbol = "\\*", doctype = FALSE,
  digits = 3, caption = "Resultados del modelo de regresión lineal.",
  caption.above = TRUE, center = TRUE,
  custom.model.names = "OLS",
  custom.coef.names = c(
    "Intercepto", "Prop. Blanco", "Ingreso prom. (log.)",
    "Partido 2012: Rep.", "Región: Nordeste", "Región: Sur",
    "Región: Oeste"
  )
)
```

#### Interpretación

Para cada coeficiente, interpretar:

- Dirección.
- Magnitud.
- Significancia estadística.

#### Predicciones

Con la ecuación y los resultados, podemos hacer predicciones sobre el valor esperado de $Y$ (proporción de votos por Trump) para distintos valores de las variables independientes. ¿Cuál es el valor esperado de $Y$ para los siguientes perfiles de condados?

- 80% blanco, en la región Sur, en un condado en el que los Republicanos ganaron en 2012- las demás variables en sus medias (ver la tabla de estadísticas descriptivas).
- 20% blanco, en el Nordeste, en un condado en el que los Demócratas ganaron en 2012- nuevamente, establecemos las demás variables en sus medias.

### Duración de gobiernos democráticos

Algunos politólogos argumentan que la variación en la duración de gobiernos democráticos en el periodo posguerra se explica por características de los gobiernos de turnos: el nivel de apoyo parlamentario de los partidos en el gobierno, el número de partidos en el mismo y la disciplina que las organizaciones partidistas pueden imponer a sus miembros. En otras palabras, tenemos el siguiente modelo teórico de la duración de un gobierno:

$$ \text{Duración gob.} = f(\text{Apoyo parl. gob.} + \text{Núm. partidos gob.} + \text{Disciplina part.} + \text{Comp. estocástico}) $$

#### Datos

Como estudiantes de Ciencia Política, buscamos datos al respecto para evaluar este argumento y nos encontramos con una base de datos recopilada por Rob Franzese hace unos años a partir del *Political Data Handbook of OECD Countries* con datos para países de la OCDE. La base de datos se encuentra en el archivo `OECD_country_data.xls` [aquí](https://github.com/josefortou/lab-book/tree/master/data). 

- Cargue los datos en R usando las funciones y librerías apropiadas para un archivo `.xls`.

Afortunadamente, el archivo también incluye un diccionario de variables (en inglés) en la segunda pestaña del archivo. Pista: `read_excel()` tiene un argumento `sheet = `.

- Identifique las variables relevantes para estimar el modelo especificado arriba y asegúrese que estén en el formato adecuado.

#### Regresión

La información contenida en esta base de datos nos permite decir algo sobre el argumento teórico sugerido al inicio.

- Utilizando estos datos, estime un modelo de regresión lineal sencillo (sin utilizar variables transformadas) que evalúe el argumento, presente los resultados en una tabla de regresión e interpretelos de forma sustancial (en términos del argumento).

#### Críticas

Frente a estos resultados, una colega sugiere que lo que está sucediendo es un poco distinto. Ella dice: "la disciplina partidista no tiene un efecto independiente sobre la duración de los gobiernos. Más bien, condiciona el efecto del apoyo legislativo al gobierno y del número de partidos en la coalición sobre la duración".

- Especifique un modelo que tome en cuenta esta crítica, estímelo via OLS e interprete los efectos de las variables independientes sobre la dependiente:

#### Extra: comparar modelos

¿Cómo adjudicamos entre estos modelos? Otra colega sugiere que una prueba *F*, realizada por medio de un análisis de varianza (ANOVA) de los dos modelos, podría decirnos algo sobre cuál es mejor.

- Realice una prueba *F* y concluya cuál modelo prefiere en términos estadísticos y teóricos:

#### Comunicar resultados

Es útil visualizar los resultados de modelos estadísticos como estos. Para el siguiente punto, seleccione el modelo que crea que se ajusta mejor a los datos y representa mejor la teoría.

- Construya una gráfica que muestre el efecto del número de partidos en la coalición de gobierno y del apoyo legislativo al gobierno sobre la duración de los mismos cuando la disciplina partidista es alta y cuando es baja. Incluya intervalos de confianza en la gráfica.

### Opinión pública sobre el presidente

Supongamos que queremos entender las percepciones y evaluaciones de individuos sobre los presidentes, particularmente los de USA. Por inspiración de las diosas politológicas, sabemos que el modelo "real" es:

$$ \text{Eval. presid.} = \alpha + \beta_1\text{Afroamericano} + \beta_2\text{ID partidista} + \beta_3\text{Ideología} + \beta_4\text{Eval. idiosincrática} + \beta_5\text{Eval. sociotrópica} + \epsilon $$

Esta es una versión de un modelo clásico en Ciencia Política, el cual considera que las evaluaciones individuales de los políticos son una función de posiciones ideológicos y percepciones de la economía. Queremos evaluar este modelo. 

#### Datos 

Nuevamente, buscamos datos y encontramos la encuesta del *American National Election Study* (ANES) para 2004, en la cual le preguntaron a los encuestados por su percepción del entonces presidente George W. Bush. Los datos se encuentran en el archivo `nes_2004_data.csv` [aquí](https://github.com/josefortou/lab-book/tree/master/data). Las principales variables son:

| Nombre de variable | Definición |
|--------------------|-----------------------------------------------------------------------------------------|
| thermom | Sentimientos por Bush (1:100; 0=mal, 100=bien) |
| race | Raza/etnicidad (10, 12, 13, 14 y 15=afroamericano) |
| partyid | Identificación partidista (0=muy Demócrata, 3=independiente, 6=muy Republicano, 7=otro) |
| libcon | ¿Liberal o conservador? (1=liberal, 3=moderado, 5=conservador, 7=no responde) |
| betteroff | Situación económica familiar el último año (1-5; 1=peor, 3=igual, 5=mejor) |
| economy | Situación económica del país el último año (1-5; 1=peor, 3=igual, 5=mejor) |
| edulevel | Nivel educativo máximo (0-7; 0=sin bachillerato, 7=posgrado) |

- Cargue el archivo de datos y realice los procedimientos necesarios para asegurarse que las variables estén debidamente codificadas.

#### Modelos

Aprovechemos que sabemos que el modelo de arriba es el "real".

- Estime el modelo "real", interprete los coeficientes y diga algo sobre la interpretación *causal* de los mismos.

- Estime el siguiente modelo -llamémoslo "modelo 2"-, que tiene una variable menos y donde los coeficientes son estimados ($\hat{\beta}$ en vez de $\beta$):

$$ \text{Eval. presid.} = \alpha + \hat{\beta_1}\text{Afroamericano} + \hat{\beta_2}\text{ID partidista} + \hat{\beta_3}\text{Eval. idiosincrática} + \hat{\beta_4}\text{Eval. sociotrópica} + \epsilon $$

- Estime este otro modelo -llamémoslo algo creativo, como "modelo 3"-, que tiene una variable más y donde los coeficientes son estimados ($\hat{\beta}$ en vez de $\beta$):

$$ \text{Eval. presid.} = \alpha + \hat{\beta_1}\text{Afroamericano} + \hat{\beta_2}\text{ID partidista} + \hat{\beta_3}\text{Ideología} + \hat{\beta_4}\text{Eval. idiosincrática} + \hat{\beta_5}\text{Eval. sociotrópica} + \hat{\beta_6}{Educación} + \epsilon $$

#### Extra: presentar resultados

- Presente los resultados de los tres modelos en una tabla y saque conclusiones sobre el *sesgo* de los coeficientes en el modelo 2 y la *eficiencia* del modelo 3.

- Presente gráficamente y discuta los efectos de todas las variables independientes en uno de los modelos.



