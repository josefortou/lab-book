# (PART) Análisis de Regresión {-}

# Regresión lineal {#cap-lm-applied}

## Resumen

En este capítulo:

- Aprendemos a estimar modelos de regresión lineal en R usando la función básica `lm()`.
- Interpretamos y presentamos los resultados usando tablas y figuras. 
- Realizamos diagnósticos a los modelos que estimamos para afinar nuestros análisis. El objetivo final es utilizar técnicas estadísticas para evaluar nuestros modelos teóricos.

**Principales conceptos**: regresión lineal, coeficiente, significancia estadística, tabla de regresión, predicciones (valores esperados), diagnósticos.
**Funciones clave**: `lm()`, `modelsummary()`, `ggpredict()`.

### Librerías

Vamos a utilizar las siguientes librerías:

```{r libraries, message=FALSE, warning=FALSE}
library(tidyverse)
library(lmtest) # diagnosticos de modelos lineales
library(sandwich) # errores estandar robustos
library(ggeffects) # efectos y predicciones en modelos de regresion
library(modelsummary) # tablas de regresion
```

```{r plot-theme, message=FALSE, warning=FALSE}
theme_set(theme_light())
```

### Datos

Debemos descargar los siguientes archivos de datos y guardarlos en la carpeta `/data` de nuestro proyecto:

- Datos de ingreso: [link](https://raw.githubusercontent.com/josefortou/lab-book/master/data/riverside_final.csv). Para descargar, hacer click derecho, "Guardar como...".

## Modelos de regresión lineal en R

Supongamos que tenemos una pregunta sobre la relación (causal) entre el ingreso de un individuo y su nivel educativo. Nuestra hipótesis es que a mayor número de años de educación, mayor ingreso anual en promedio. Buscamos y encontramos datos al respecto (y los guardamos en el archivo `riverside_final.csv`, disponible [aquí](https://raw.githubusercontent.com/josefortou/lab-book/master/data/riverside_final.csv) y los cargamos en R: 

```{r cargar-datos}
# cargar datos
riverside <- read_csv("data/riverside_final.csv")
```

Reorganizamos un poco los datos -recodificamos unas variables- y miramos las primeras filas:

```{r ver-datos}
# recodificar variables categóricas
riverside <- riverside %>%
  mutate(
    gender = factor(gender, levels = c(0, 1), labels = c("Mujer", "Hombre")),
    party = factor(party, levels = c(0, 1, 2), labels = c("Rep.", "Dem.", "Ind."))
  )

# imprimir los datos a la consola
head(riverside)
```

Vemos que tenemos información sobre ingreso, nivel educativo, experiencia laboral, género y afiliación partidista para `r tally(riverside)` individuos. Podemos empezar a trabajar.

## Análisis exploratorio y visualización

Empezamos haciendo un poco de análisis exploratorio de datos. Primero, veamos un resumen de las variables usando la función `summary()`:

```{r summary-data}
summary(riverside)
```

Usemos la función `datasummary_skim()` de la librería `modelsummary` para ver un resumen similar, pero con información adicional:

```{r datasummary}
datasummary_skim(riverside)
```

Esto solo nos muestra un resumen de las variables numéricas. Si tenemos variables categóricas (definidas como tipo `factor` o `character` en R), lo especificamos en la función:

```{r skim-cat}
datasummary_skim(riverside, type = "categorical")
```

`modelsummary` también sirve para ver correlaciones entre variables numéricas:

```{r cor-table}
datasummary_correlation(riverside)
```

Como nuestras principales variables de interés (ingreso y educación) son numéricas, un gráfico de dispersión de ambas es apropiado para evaluar la relación entre ellas:

```{r scatter1}
riverside %>%
  ggplot(aes(edu, income)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Educación (años)", y = "Ingreso anual (USD)")
```

Parece que hay una relación lineal positiva entre los años de educación y el ingreso anual de un individuo. Sin embargo, después de revisar la literatura, creemos que la experiencia laboral y el género de un individuo también impactan su ingreso. Así que exploramos la relación entre la experiencia profesional (en años) de cada individuo y su ingreso:

```{r scatter2}
riverside %>%
  ggplot(aes(senior, income)) +
  geom_point() +
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Experiencia laboral (años)", y = "Ingreso anual (USD)")
```

Vemos un patrón similar. Además, podemos mirar la relación entre género -una variable categórica binaria en nuestros datos- e ingreso:

```{r boxplot}
riverside %>%
  ggplot(aes(gender, income)) +
  geom_boxplot() + # caja y bigote
  scale_y_continuous(labels = scales::dollar) +
  labs(x = "Género", y = "Ingreso anual (USD)")
```

Para que un análisis juicioso de las causas del ingreso a nivel individual debe considerar más de un aspecto.

## Análisis de regresión

Basados en nuestra teoría y la literatura, creemos que el ingreso de un individuo $i$ es una función lineal del nivel de educación, la experiencia laboral y el género de dicho individuo. Entonces, la ecuación del modelo de regresión que vamos a estimar es el siguiente:

$$ \hat{\text{Ingreso}}_i = \hat{\alpha} + \hat{\beta}_1 \text{Educación}_i + \hat{\beta}_2 \text{Experiencia}_i + \hat{\beta}_3 \text{Hombre}_i + \hat{u}_i $$

### Estimación

En R, estimamos modelos de regresión lineal usando la función `lm()`. Esta función toma como principal argumento una fórmula de la forma `y ~ x + z`, donde `y` es la variable dependiente y `x` y `z` son variables independientes.

Estimemos un modelo simple ($\text{Ingreso}_i = \alpha + \hat{\beta} \text{Educación}_i$) y guardémoslo como un objeto:

```{r mod-simple}
modelo_simple <- lm(income ~ edu, data = riverside)
```

Así de fácil estimamos nuestra primera regresión lineal. Si queremos explorar los resultados del modelo (los coeficientes estimados, por ejemplo), podemos usar la función `summary()`:

```{r}
summary(modelo_simple)
```

Vemos varios elementos, incluyendo coeficientes (`Estimate`), errores estándar (`Std. Error`) y *p-values* (`Pr(>|t|)`) de cada variable, más unas estadísticas del ajuste del modelo en general (como el $R^2$). Estos elementos se pueden extraer individualmente. Por ejemplo, si quiero ver solo los coeficientes:

```{r}
coef(modelo_simple)
```

Por ahora, nos vamos a enfocar en estos coeficientes. Recordemos brevemente la interpretación básica de los coeficientes de un modelo de regresión lineal: 

- El intercepto ($\alpha$) es el valor esperado de $Y$ (ingreso, en este caso) cuando todas las variables independientes $= 0$.
- El coeficiente de regresión de una variable independiente ($\beta$) es el incremento en el valor esperado de $Y$ (ingreso) asociado a un incremento de *una unidad* en $X$ (educación).

Así, basados en este modelo, concluimos que un aumento de 1 año en el nivel educativo de un individuo se asocia con un incremento de aproximadamente 2651 USD anuales en su ingreso. 

Podemos estimar modelos de regresión con variables independientes categóricas, por ejemplo binarias (también llamadas variables *dummy* o variables indicador). Usemos la variable `gender` que toma dos valores en los datos: "Hombre" y "Mujer". Estimemos el modelo con la misma función, pero cambiando la variable independiente, y veamos los resultados:

```{r mod-dummy}
modelo_genero <- lm(income ~ gender, data = riverside)
summary(modelo_genero)
```

Recordemos que cuando se trata de variables independientes binarias, la interpretación de los coeficientes cambia levemente:

- $\alpha$ es el valor esperado de $Y$ cuando el valor de $X$ es la categoría de referencia - en este caso, $\hat{Y}$ cuando $X = \text{Mujer}$. 
- $\beta$ es la diferencia en el valor esperado de $Y$ para la(s) otra(s) categoría(s) de la variable categórica. En este caso, cuando $X = 1$ (hombre), $\hat{Y} = \hat{\alpha} + \hat{\beta}$. Cuando $X = 0$ (mujer), entonces $\hat{Y} = \hat{\alpha}$.

Así, basados en este modelo, concluimos que un aumento de 1 año en el nivel educativo de un individuo se asocia con un incremento de aproximadamente 2651 USD anuales en su ingreso. 

El uso de variables independientes categóricas binarias es fácilmente extendible a variables categóricas multinominales u ordinales, pues podemos conceptualizarlas como una serie de variables *dummy*. La e

Veamos qué pasa cuando utilizamos la variable "identificación partidista" (`party`) como variable independiente. Esta variable toma tres valores: "Rep.", "Dem.", o "Ind.". La ecuación sería:

$$ \hat{\text{Ingreso}}_i = \hat{\alpha} + \hat{\beta}_1 \text{Dem.}_i + \hat{\beta}_2 \text{Ind.}_i + \hat{u}_i $$

Donde "Dem." e "Ind." son variables dummy que pueden tomar los valores 0 o 1, para quienes pertecen a ese grupo (1) o no (0). Como antes, la categoría omitida es la de referencia y el intercepto la "absorbe": $\hat\alpha$ es el ingreso esperado para los "Rep.". 

En la práctiva, no tenemos que crear estas variable "dummy" manualmente; nuestro software las crea automáticamente. Entonces, estimamos el modelo en R de la siguiente manera y obtenemos estos resultados:

```{r mod-cat}
modelo_party <- lm(income ~ party, data = riverside)
summary(modelo_party)
```

Vemos dos coeficientes para `party`: uno para individuos que se identifican como "Dem." y otro para "Ind.". No hay coeficiente para "Rep.", lo cual indicica que es la categoría de referencia o base. La interpretación es análoga:

- $\alpha$ es el valor esperado de $Y$ cuando el valor de $X$ es la categoría de referencia - en este caso, $\hat{Y}$ cuando $X = \text{Rep.}$. 
- $\beta_1$ y $\beta_2$ son la diferencia en el valor esperado de $Y$ cuando comparamos las categorías correspondientes con la de referencia. En este caso, cuando $X = \text{Dem.}$, $\hat{Y} = \hat{\alpha} + \hat{\beta}_1$. Cuando $X = \text{Ind.}$ (Rep.), entonces $\hat{Y} = \hat{\alpha} + \hat{\beta}_2$.

¿Pero podemos decir que estas asociaciones es una buena aproximación al *efecto* de la educación o del género sobre el ingreso? Sabemos que hay variables omitidas, lo cual introduce sesgo. Podemos mejorar la situación usando modelos de regresión múltiples.

#### Regresión múltiple

Sin embargo, dado nuestro modelo teórico, nos interesa estimar un modelo de regresión **múltiple** con más de una variable independiente. Para esto,  agregamos más variables independientes a la fórmula con el operador `+`. Estimamos el modelo, lo guardamos como un objeto e imprimimos a la consola un resumen de los resultados:

```{r mod-completo}
modelo_ingreso <- lm(income ~ edu + senior + gender + party, data = riverside)
summary(modelo_ingreso)
```

### Interpretación

Esta información es suficiente para interpretar tres elementos básicos: la **magnitud**, **dirección** y **significancia estadística** de los coeficientes. La única diferencia es que ahora interpretamos los coeficientes "manteniendo todo lo demás constante" (*ceteris paribus*) o "controlando por" las demás variables. Así, basados en la tabla anterior:

- El coeficiente `edu` es el cambio en `income` asociado a un incremento de 1 unidad en `edu`, *ceteris paribus*.
- El coeficiente `senior` es el cambio en `income` asociado a un incremento de 1 unidad en `senior`, *ceteris paribus*.
- El coeficiente `genderHombre` es la diferencia en `income` entre hombres y mujeres (la categoría de referencia), *ceteris paribus*.
- El coeficiente `partyDem.` es la diferencia en `income` entre demócratas y republicanos (la categoría de referencia), *ceteris paribus*.
- El coeficiente `partyInd.` es la diferencia en `income` entre independientes y republicanos (la categoría de referencia), *ceteris paribus*.

El signo (positivo o negativo) en cada coeficiente indica si la relación es positiva o negativa. Así, mientras que un aumento en el nivel educativo tiene una asociación positiva con ingreso (si la educación aumenta, el ingreso aumenta), los demócratas tienen menos ingreso, comparados con los republicanos.

El valor *p* (o *p-value*) para cada coeficiente nos indica la probabilidad de observar ese coeficiente ($\hat\beta$, que estimamos) si el coeficiente "real" ($\beta$, en la población) es igual a 0. Convencionalmente, si es probabilidad es baja -típicamente $p < 0.05$- decimos que el coeficiente asociado es estadísticamente significativo.

Volviendo a nuestros resultados, podemos hacer la siguiente interpretación en términos sustanciales que:

- Manteniendo las demás variables constantes, un año adicional de educación se asocia en promedio con un incremento de `r coef(modelo_ingreso)[[2]] %>% round()` USD en el ingreso anual.
- Manteniendo las demás variables constantes, un año adicional de experiencia laboral se asocia en promedio con un incremento de `r coef(modelo_ingreso)[[3]] %>% round() * 0.01` USD en el ingreso anual.
- Manteniendo las demás variables constantes, el ingreso anual de un hombre es en promedio `r coef(modelo_ingreso)[[4]] %>% round()` USD mayor que el de una mujer.
- Manteniendo las demás variables constantes, el ingreso anual de un demócrata es en promedio `r coef(modelo_ingreso)[[5]] %>% round()` USD menor que el de un republicano.
- Manteniendo las demás variables constantes, el ingreso anual de un independiente es en promedio `r coef(modelo_ingreso)[[4]] %>% round()` USD mayor que el de un republicano.

Los tres primeros coeficientes son estadísticamente significativos ($p < 0.05$), lo cual indica que la probabilidad de observar estos valores si la hipótesis nula ($\beta = 0$) fuese cierta es menos del 5%. O sea, es tan poco probable observar estos coeficientes, que ya que los observamos, debe ser que la hipótesis nula no representa el mundo real. En el mundo en que la hipótesis nula es cierta, $\beta = 0$ -- el *p-value* nos dice si el $\beta$ que observamos/estimamos encaja en un mundo en que $\beta = 0$. No podemos decir lo mismo de los coeficientes asociados a la indentidad partidista; por tanto, no hay una relación clara entre esta variable y el ingreso.

<!-- Luego, volveremos a este modelo para discutir cómo interpretarlo. -->

<!-- #### Especificación del modelo -->

<!-- Además de añadir términos (variables independientes) a un modelo, podemos también cambiar la forma en que las variables entran al modelo. Dos transformaciones comunes son los logaritmos y los términos multiplicativos. -->

<!-- ##### Logaritmos -->

<!-- La regresión lineal está diseñada para funcionar mejor con variables dependientes con distribuciones normales (o cercanas a la normal). Cuando encontramos una variable con una distribución sesgada hacia la derecha (muchos casos con valores bajos, pocos con valores altos), aplicarle la función logarítmica a la variable dependiente permite mover la distribución de forma tal que se acerque más a la normal. Como la función logarítmica aplica una transformación afín, no cambia la estructura subyacente de los datos.  -->

<!-- <!-- Por estas razones metodológicas y teóricas, las variables en escala logarítmica son comunes en modelos de regresión. La siguiente gráfica muestra lo que sucede cuando aplicamos la función logarítmica a una variable con una distribución sesgada hacia la derecha (en este caso, una variable con distribución Gamma): --> -->

<!-- <!-- ```{r distrib-log} --> -->
<!-- <!-- tibble( --> -->
<!-- <!--   variable_gamma = rgamma(10000, 3), --> -->
<!-- <!--   variable_log_normal = log(variable_gamma) --> -->
<!-- <!-- )  %>% --> -->
<!-- <!--   pivot_longer(variable_gamma:variable_log_normal,  --> -->
<!-- <!--                names_to = "distrib", names_prefix = "variable_", --> -->
<!-- <!--                values_to = "valores") %>% --> -->
<!-- <!--   ggplot(aes(valores)) + --> -->
<!-- <!--   geom_histogram()+ --> -->
<!-- <!--   facet_wrap(~ distrib, scales = "free", ncol = 1) + --> -->
<!-- <!--   labs(x = "Valores", y = "Número de observaciones") --> -->
<!-- <!-- ``` --> -->

<!-- Podríamos pensar que la asociación entre experiencia laboral e ingreso no es lineal, sino que exhibe rendimientos decrecientes: pasar de 0 a 5 años de experiencia se asociaría con un aumento en el ingreso mayor que pasar de 10 a 15 años de experiencia. Es posible que una transformación logarítmica de la variable `senior` nos permita capturar intuiciones teóricas como los rendimientos decrecientes. Recordemos la forma de una función logarítmica para hacer la conexión con esta idea teórica: -->

<!-- ```{r} -->
<!-- tibble( -->
<!--   x = 1:100, -->
<!--   log_x = log(x) -->
<!-- ) %>% -->
<!--   ggplot(aes(x, log_x)) + -->
<!--   geom_line(color = "red") -->
<!-- ``` -->

<!-- Para valores bajos de $x$, el aumento correspondiente de $\log(x)$ es mayor que el aumento cuando los valores de $x$ son mayores. -->

<!-- Ahora, exploremos la relación entre el logaritmo natural de la experiencia en años y el ingreso: -->

<!-- ```{r scatter-log} -->
<!-- riverside %>% -->
<!--   ggplot(aes(log(senior), income)) + -->
<!--   geom_point() + -->
<!--   scale_y_continuous(labels = scales::dollar) + -->
<!--   scale_x_continuous(trans = "log") + -->
<!--   labs(x = "Experiencia laboral (años), log.", y = "Ingreso anual (USD)") -->
<!-- ``` -->

<!-- Podemos estimar el logaritmo (natural) y usarlo directamente en un modelo en la fórmula de `lm()` con la función `log()`: -->

<!-- ```{r mod-log} -->
<!-- lm(income ~ edu + log(senior) + gender, data = riverside) %>% -->
<!--   summary() -->
<!-- ``` -->

<!-- Sin embargo, es preferible realizar la transformación antes, para poder tener información (como por ejemplo, estadísticas resumen) de la variable transformada, como lo hicimos arriba - aquí repetimos la operación como demostración: -->

<!-- ```{r mutate-log} -->
<!-- riverside <- riverside %>% -->
<!--   mutate(senior_log = log(senior)) -->
<!-- ``` -->

<!-- Para estimar el modelo con la variable de experiencia en versión logaritmo, simplemente la agregamos a la fórmula en `lm()`: -->

<!-- ```{r mod-log2} -->
<!-- modelo_log <- lm(income ~ edu + senior_log + gender, data = riverside) -->
<!-- summary(modelo_log) -->
<!-- ``` -->

<!-- Recordemos que en este caso, los demás coeficientes y el intercepto se interpretan igual, pero el coeficiente para la variable transformada se interpreta de manera diferente: un aumento de $1\%$ en $X$ (ojo, no en $\log(X)$) se asocia con un incremento de $\hat{\beta} \times 0.01$ en $\hat{Y}$.  -->

<!-- Es posible también transformar la variable dependiente de manera logarítmica (un modelo log-normal) o tanto la dependiente como una independiente (un modelo log-log). La interpretación en esos casos también cambia, pero no la discutimos aquí. -->

<!-- ##### Términos multiplicativos o interacciones -->

<!-- Supongamos que creemos que la asociación entre $X_1$ y $Y$ (el coeficiente $\beta_1$) depende del valor de $X_2$.^[O, a la inversa, que la asociación entre $X_2$ y $Y$ (el coeficiente $\beta_2$) depende del valor de $X_1$.] En otras palabras, creemos que hay una variable que modera la asociación entre $X_1$ y $Y$. En este caso, podemos estimar un modelo lineal con términos multiplicativos o interacciones entre las variables independientes. Las interacciones nos permiten capturar una noción teórica interesante que nos permiten capturar esta idea.  -->

<!-- En términos prácticos, una interacción es una multiplicación entre dos variables. Pueden ser dos variables numéricas, dos categóricas (recordemos que para efectos de estimar modelos, las categorías se convierten en enteros) o una numérica y una categórica. La interacción más común es la interacción entre una variable numérica y una categórica. En este caso, podríamos pensar que la relación entre educación (una variable numérica) e ingreso es distinta para hombres y para mujeres (categorías). Incorporamos esta noción teórica al modelo que vamos a estimar: -->

<!-- $$ \hat{\text{Ingreso}}_i = \hat{\alpha} + \hat{\beta}_1 \text{Educación}_i + \hat{\beta}_2 \text{Hombre}_i + \hat{\beta}_3 (\text{Educación}_i \times \text{Hombre}_i) + \hat{u}_i $$ -->

<!-- En R, simplemente multiplicamos los dos términos en la fórmula de `lm()` - esto se puede hacer antes y crear una nueva columna en los datos: -->

<!-- ```{r mod-inter} -->
<!-- modelo_inter <- lm(income ~ edu*gender, data = riverside) -->
<!-- summary(modelo_inter) -->
<!-- ``` -->

<!-- Noten que el modelo incluye tanto el término multiplicativo (`edu:genderHombre`), como los componentes del mismo; esto es por diseño. La interpretación de los coeficientes de las variables que no hacen parte de una interacción permanece igual, pero la forma en que interpretamos la asociación entre $X_1$ y $Y$ y entre $X_2$ y $Y$ cambia de manera importante: -->

<!-- - Cuando $X_2 = 0 = \text{"Mujer"}$, el intercepto es $\alpha$ y la asociación entre $X_1$ y $Y$ es $\beta_1$. -->
<!-- - Cuando $X_2 = 1 = \text{"Hombre"}$, el intercepto es $\alpha + \beta_2$ y la asociación entre $X_1$ y $Y$ es $\beta_1 + \beta_3$. -->

### Diagnósticos

<!-- Hagamos unos diagnósticos básicos del ajuste y los supuestos de nuestro modelo de regresión lineal. Empecemos por extraer algunos elementos del modelo estimado y organizándolos en un `tibble`: -->

<!-- ```{r augment-mod} -->
<!-- # datos (observaciones y variables) usados en el modelo -->
<!-- modelo_aug <- as_tibble(modelo_ingreso$model) -->
<!-- modelo_aug <- modelo_aug %>% -->
<!--   mutate(fitted = modelo_ingreso$fitted.values, -->
<!--          residuals = modelo_ingreso$residuals, -->
<!--          std_residuals = rstandard(modelo_ingreso), -->
<!--          cooks_d = cooks.distance(modelo_ingreso)) -->
<!-- modelo_aug -->
<!-- ``` -->


<!-- Para entender un poco mejor si nuestro modelo captura los patrones subyacentes en los datos, podemos mirar una gráfica de dispersión de las variables independientes numéricas versus los residuos. En este caso, como no vemos ningún patrón evidente, creemos que el modelo captura señal y no solo ruido: -->

<!-- ```{r resid-x} -->
<!-- modelo_aug %>% -->
<!--   ggplot(aes(residuals, edu)) +  -->
<!--   geom_point() + -->
<!--   labs(x = "Educación (años)", y = "Residuos.") -->
<!-- modelo_aug %>% -->
<!--   ggplot(aes(residuals, senior)) +  -->
<!--   geom_point() + -->
<!--   labs(x = "Experiencia (años)", y = "Residuos") -->
<!-- ``` -->

El modelo de regresión lineal por mínimos cuadrados ordinarios (MCO u OLS) es el estimador lineal insesgado de mínima varianza ("OLS is BLUE") cuando se cumplen una serie de supuestos, entre ellos, supuestos sobre los errores $u$ y su distribución. R nos permite realizar unos diagnósticos para evaluar si estos supuestos se cumplen. 

#### Linealidadad

En OLS, asumimos que $Y$ es una función lineal de las variables independientes. Evaluamos este supuesto visualmente con una gráfica de los valores esperados del modelo (las predicciones) vs. los residuos del mismo, buscando si hay una relación entre ambos:

```{r linear-plot}
plot(modelo_ingreso, which = 1)
```

Si vemos una relación clara (¡miren la línea roja!), hay un problema de linealidad. Violar este supuesto es un problema de específicación del modelo, o sea qué variables incluimos y de qué forma entran. En este caso, podríamos transformar nuestras variables independientes  con logaritmos o funciones exponenciales.

#### Normalidad

OLS asume que los residuos (errores) del modelo están distribuidos normalmente. Nuevamente, realizamos una evaluación visual del supuesto (una gráfica Q-Q), buscando que los puntos no se desvíen mucho de la línea diagonal, especialmente en los extremos:

```{r}
plot(modelo_ingreso, which = 2)
```

Otra opción es usar un prueba estadística, como el test de Shapiro o el Kolmogorov-Smirnoff, vía las funciones `shapiro.test()`. Si obtenemos un *p-value* menor a 0.05, rechazamos la hipótesis nula de normalidad en la distribución de los residuos:

```{r}
modelo_ingreso %>%
  resid() %>%
  shapiro.test()
```

Cuando vemos violaciones muy severas de este supuesto, puede que haya un problema con nuestra variable dependiente. Quizás en realidad sea una medida binaria, categórica u ordinal con pocos niveles. En estos casos, podemos buscar medidas alternativas u técnicas distintas a la regresión lineal por OLS.

#### Independencia

Un supuesto muy importante es que las observaciones son sacadas de una muestra de manera independiente; en otras palabras, que no hay patrones de dependencia espacial, temporal o multinivel entre observaciones. Este es un supuesto que podemos evaluar simplemente conociendo nuestros datos y teoría. ¿Mis casos son países que observo año tras año? ¿O quizás espero que lo que sucede en un municipio afecte a los vecinos? Esas son situaciones de autocorrelación temporal y/o espacial. Otra forma de evaluar este supuesto es viendo mapas o series de tiempo de nuestra variable dependiente en búsqueda de tendencias.

Sin embargo, también existen pruebas estadísticas para evaluar el supuesto de independencia. El test de Durbin-Watson, implementado con la función `dwtest()` de la librería `lmtest`, es una de estas pruebas:

```{r}
dwtest(modelo_ingreso)
```

Un *p-value* por encima de 0.05 es consistente con la hipótesis nula de no autocorrelación. 

Violaciones al supuesto de independencia se solucionan estimando un modelo distinto; en algunas ocasiones, *muy* distinto. Estos son temas avanzados que no cubrimos aquí, pero más adelante discutimos los estimadores de efectos fijos para analizar datos en panel.

#### Homoesquedasticidad

El supuesto de homoesquedasticidad significa que la varianza de los residuos es constante. Violar este supuesto -los errores están correlacionados entre sí, por ejemplo, y la varianza de los mismos no es constante- implica que los errores estándar estarían mal estimados (más pequeños de lo que deberían ser), dándonos una falsa sensación de confianza.

Para evaluar este supuesto, podemos construir un gráfico de valores esperados de $Y$ versus los residuos estandarizados (en realidad, la raíz cuadrada de los valores absolutos de estos errores). Cuando se cumple el supuesto, no debemos observar ninguna relación clara entre ambos valores:

```{r hetero-plot}
plot(modelo_ingreso, which = 3)
```

Buscamos que a) la línea roja sea aproximadamente horizontal y b) la dispersión de los puntos no cambie mucho en función de los valores esperados. Aquí parece que tenemos un problema de heteroesquedasticidad.

Un test de Breusch-Pagan nos permitiría ponerle más precisión a este diagnóstico. Usamos la función `bptest()`, nuevamente de la librería `lmtest`. Veamos:

```{r}
bptest(modelo_ingreso)
```

La hipótesis nula en esta prueba es homoesquedasticidad. Si la rechazamos con un *p-value* bajo (menor a 0.05), tenemos heteroesquedasticidad. 

<!-- El test evalúa si hay homoesquedasticidad. En este caso, el *p-value* por encima de 0.05 significa que no rechazamos la hipótesis nula de homoesquedasticidad: -->

<!-- ```{r bp-test} -->
<!-- bptest(modelo_ingreso) -->
<!-- ``` -->

<!-- Si en cambio $p < 0.05$, rechazamos la hipótesis nula de homoesquedasticidad e inferimos que hay heteroesquedasticidad en nuestro modelo. Esto implica que los nuestros errores estándar del modelo pueden ser menores de lo que deberían ser. Existen ajustes para este problema, como usar "Weighted Least Squares" (WLS) o transformar las variables independientes. -->

Una de las razones por las que podemos encontrar heteroesquedasticidad en un modelo es que tenemos observaciones con mucha influencia sobre los resultados; podríamos decir que son atípicas. La $D$ de Cook es una estadística que nos permite medir la influencia de una observación en un modelo de regresión lineal. Vamos a calcular esta estadística para cada una de las observaciones que utilizamos en nuestro modelo y vamos a colocarlas junto a dichas observaciones en un `tibble`:

```{r cook-d}
datos_modelo <- bind_cols(
  modelo_ingreso$model,
  d_cook = cooks.distance(modelo_ingreso)
)
head(datos_modelo)
```

Ahora, podemos buscar y eliminar las observaciones con una $D$ de Cook alta. Dos reglas informales sugieren que observaciones con valores por encima de 1 o con valores por encima de 3 veces la media pueden tener alta influencia. Vamos a eliminar estas observaciones, reestimar el modelo y volver a evaluar el supuesto de homoesquedasticidad:

```{r remove-outliers}
datos_modelo %>%
  filter(d_cook < 3*mean(d_cook, na.rm = TRUE)) %>%
  lm(income ~ edu + senior + gender, data = .) %>%
  plot(., which = 3)
```

Parece que no mejoró mucho la situación. 

##### Errores estándar "robustos"

Otra opción para lidiar con los problemas asociados con la heteroesquedasticidad es corregir los errores estándar después de estimar el modelo y diagnosticarlo. Después de todo, sabemos que ese es el efecto de violar el supuesto. Vamos a utilizar la función `coeftest()` de la librería `sandwich` para recalcular los errores estándar. El resultado son lo que llamamos "erorres estándar robustos", "errores consistentes cono heteroesquedasticiad" o "errores de Huber-White":

```{r}
coeftest(modelo_ingreso, vcov = vcovHC)
```

Noten que los errores estándar de las variables `edu` y `senior` ahora son más grandes. La práctica de estimar errores estándar robustos de distintos tipos es bastante común en Ciencia Política y Economía. Más adelante, los incorporamos a nuestra presentación de resultados.

Hay otras soluciones al problema de heteroesquedasticidad, como aplicar *weighted least squares* (WLS) o usar técnicas como el *bootstrap*. Por el momento, procedemos con nuestro análisis, estimamos errores robustos si es el caso y reportamos nuestros resultados. 

<!-- La librería `broom` ofrece varias funciones para trabajar con modelos estadísticos. Primero, la función `tidy()` nos permite ver los coeficientes y su significancia en forma de `tibble`: -->

<!-- ```{r mod-tidy} -->
<!-- tidy(modelo_ingreso) -->
<!-- ``` -->


<!-- Por otro lado, la función `summary()` también nos ofrece una mirada al ajuste del modelo en general. Entre otras estadísticas de "bondad del ajuste", el $R^2$ indica la proporción de la variación de $Y$ explicada por el modelo. A menos que estemos haciendo predicciones o comparando modelos similares que utilizan los mismos datos, no vale la pena dedicarle mucho tiempo al $R^2$: -->

<!-- ```{r mod-glance} -->
<!-- summary(modelo_ingreso) -->
<!-- ``` -->

<!-- Finalmente, la función `augment()` toma los datos utilizados en el modelo y los *aumenta* con los resultados del modelo, agregando el valor esperado de $Y$, el error estándar y los residuos $u$ para cada observación utilizada para estimar el modelo: -->

<!-- ```{r mod-augment} -->
<!-- modelo_aug <- augment(modelo_ingreso) -->
<!-- modelo_aug -->
<!-- ``` -->

<!-- Podemos utilizar este objeto para entender la diferencia entre el valor observado de la variable dependiente (`income`) y el valor esperado o la predicción del modelo (`.fitted`). Esas diferencias son los residuos. Por ejemplo, miremos la primera observación en la base de datos: -->

<!-- ```{r resid} -->
<!-- modelo_ingreso %>% -->
<!--   augment() %>% -->
<!--   slice(1) %>% # seleccionar la primera fila -->
<!--   select(edu, senior, gender, income, .fitted, .resid)  -->
<!-- ``` -->

<!-- Vemos que el modelo se equivoca (especificamente, predice menos ingreso del observado). Eso es normal. El modelo de regresión lineal estima la ecuación que mejor describe la asociación entre variables *en promedio*. -->

## Predicciones y efectos

Hasta ahora, hemos discutido cómo interpretar los coeficientes de un modelo de regresión simple y múltiple con distintos tipos de variables independientes (numéricas, binarias y multinominales), utilizando la función `summary()` para ver estos resultados. Pero podemos refinar un poco más la interpretación usando el modelo de regresión como herramienta predictiva, lo cual nos permite ver mejor cambios en la variable dependiente cuando las variables independientes toman distintos valores.

Una de las herramientas más potentes que nos da un modelo de regresión lineal es la capacidad de hacer predicciones. Una predicción no es más que el valor esperado de $Y$ para determinados valores de las variables independientes. Hallamos estos valores esperados o predicciones utilizando los coeficientes estimados y reemplazando los valores de las variables independientes en la ecuación del modelo de regresión lineal.

Calculemos un valor esperado "a mano" usando nuestro modelo. Primero, vamos a extraer los coeficientes del modelo usando `coef()` y `pluck()` - están en el orden en que aparecen en el resumen del modelo:

```{r}
alfa <- coef(modelo_ingreso) %>% pluck(1)
beta_edu <- coef(modelo_ingreso) %>% pluck(2)
beta_exp <- coef(modelo_ingreso) %>% pluck(3)
beta_hombre <- coef(modelo_ingreso) %>% pluck(4)
beta_dem <- coef(modelo_ingreso) %>% pluck(5)
beta_ind <- coef(modelo_ingreso) %>% pluck(6)
```

Ahora, vamos a seleccionar valores de las variables independientes que nos interesan:

```{r}
intercepto <- 1 # siempre es 1
edu <- 10 # 10 años de educacion
exp <- max(riverside$senior, na.rm = TRUE) # ejecutiva
gen <- 0 # mujer
dem <- 1 # demócrata
ind <- 0
```

Finalmente, reemplazamos para hallar el valor esperado de $Y$:

```{r}
intercepto + beta_edu*edu + beta_exp+exp + beta_hombre*gen + beta_dem*dem + beta_ind*ind
```

Comparemos con nuestra predicción para un hombre, manteniendo las demás características igual:

```{r}
gen <- 1
intercepto + beta_edu*edu + beta_exp+exp + beta_hombre*gen + beta_dem*dem + beta_ind*ind
```

¡La diferencia entre ambos valores esperados es exactamente el coeficiente de la variable género, `r coef(modelo_ingreso) %>% pluck(4)`!

Con esta información, podemos hacer inferencias sobre la relación entre las variables de interés. En otras palabras, podemos ver "efectos", *asumiendo que hemos superado los distintos obstáculos a la inferencia causal* (especialmente, que no hay sesgo por variables omitidas).

Podemos automatizar, o si se quiere "tercerizar", este proceso. Digamos que estamos interesados en conocer el ingreso esperado de una mujer con educación y experiencia promedio en comparación con un hombre con las mismas cualificaciones. Vamos a usar la librería `ggeffects`, específicamente la función `ggpredict()`. Tomamos el modelo, lo pasamos a la función y especificamos las variables y si queremos, los valores que nos interesan. La función mantiene las variables numéricas no especificadas constantes en la media y mantiene las categóricas en su valor de referencia (es importante entonces recodificar bien):

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("gender"))
```
Vemos entonces el ingreso esperado para hombres y mujeres con 16 años de educación, 14.81 años de experiencia y con afiliación republicana.

O de pronto nos interesa comparar el valor esperado del ingreso de individuos cuando se tiene poca y mucha experiencia. Usamos la opción `[minmax]` para ver el mínimo y el máximo de la variable independiente de interés:

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("senior[minmax]"))
```

En este caso, vemos el ingreso esperado para individuos con poca y mucha experiencia (1 y 27 años) con las siguientes características: 16 años de mujer, mujeres, republicanas.

Podemos incluir más variables independientes (hasta cuatro en `ggeffects`):

```{r}
modelo_ingreso %>%
  ggpredict(terms = c("gender","senior[minmax]"))
```

Esta es una herramienta potente para interpretar los resultados de un modelo de regresión lineal.

<!-- Por ejemplo, podríamos estar interesados en el ingreso esperado de una mujer con educación y experiencia promedio. Una forma de hacer esto es usando las funciones de `modelr`. Ponemos esta información en una tabla con `expand_grid()` y la pasamos a la función `add_predictions()`, especificando el modelo que queremos usar para hallar las predicciones: -->

<!-- ```{r pred} -->
<!-- riverside %>% -->
<!--   data_grid( -->
<!--     edu = mean(riverside$edu),  -->
<!--     senior = mean(riverside$senior), -->
<!--     gender = "Mujer" -->
<!--   ) %>% -->
<!--   add_predictions(modelo_ingreso)  -->
<!-- ``` -->

<!-- Otra forma de hacer esto de manera programática es si creamos una tabla que contenga combinaciones de las variables independientes y luego, con `add_predictions` agregamos los valores esperados de $Y$ (predicciones) para esas combinaciones. Aquí, por ejemplo, vamos de una unidad en una unidad por todo el rango de educación, mantenemos experiencia constante en la mediana e incluimos todos los valores de la variable binaria género: -->

<!-- ```{r pred2} -->
<!-- riverside %>% -->
<!--   data_grid( -->
<!--     edu = seq_range(edu, by = 1), # secuencia en el rango de la variable -->
<!--     senior = median(senior),  -->
<!--     gender  -->
<!--   ) %>% -->
<!--   add_predictions(modelo_ingreso) -->
<!-- ``` -->

<!-- Posteriormente, podemos filtrar el resultado para ver predicciones puntuales o usarlos para presentar resultados gráficamente (próxima sección). -->

## Presentar resultados

Miremos dos formas principales de presentar los resultados de una regresión: tablas y gráficas. Vamos a utilizar las librerías `ggeffects` y `modelsummary` para esto. Hay muchas más opciones, como `margins`, `sjPlot`, `gtsummary`, `stargazer`, etc.

### Tablas de regresión

Casi todos los artículos que utilizan modelos de regresión presentan los resultados en tablas. En ellas, típicamente vemos los coeficientes, intervalos de confianza o errores estándar, la significancia de los coeficientes (con *p-values*), el número de observaciones y algunas medidas del ajuste del modelo, como el $R^2$.

Para hacer tablas de regresión, usamos la función `modelsummary()` de `modelsummary`. Le pasamos nuestro modelo y la función arroja una tabla formateada:

```{r tab-simple}
modelsummary(modelo_ingreso)
```

Los argumentos de la función nos permiten cambiar muchos de los elementos de la tabla, como ponerle etiquetas a los coeficientes, presentar intervalos de confianza en vez de errores estándar y agregar un título:

```{r tab-options}
modelsummary(
  modelo_ingreso,
  coef_map = c(
    "(Intercept)" = "(Intercepto)",
    "edu" = "Educación",
    "senior" = "Experiencia",
    "genderHombre" = "Género: hombre"
  ),
  statistic = "conf.int",
  title = "Resultados del análisis."
)
```

Una tabla de regresión puede presentar varios modelos lado a lado para efectos de comparación. Si ponemos los modelos que estimamos antes en una lista (y, como bono, les damos nombres), podemos incluirlos en una misma tabla:

```{r tab-multi}
lista_modelos <- list(
  "Educación" = modelo_simple,
  "Género" = modelo_genero,
  "Completo" = modelo_ingreso
)
modelsummary(
  lista_modelos, # varios modelos
  coef_map = c(
    "(Intercept)" = "(Intercepto)",
    "edu" = "Educación",
    "senior" = "Experiencia",
    "genderHombre" = "Género: hombre",
    "partyDem." = "Afil.: dem.",
    "partyInd." = "Afil.: ind."
  )
)
```
Hay varias opciones adicionales. Por ejemplo, podemos indicar niveles de significancia estadística con la opción `stars = TRUE`. Adicionalmente, si queremos reportar errores estándar robustos, está la opción `vcov = ...` que trabaja en conjunto con las funciones de la librería `sandwich`. Así, para errores estándar robustos "básicos":

```{r tab-multi2}
modelsummary(
  lista_modelos, # varios modelos
  coef_map = c(
    "(Intercept)" = "(Intercepto)",
    "edu" = "Educación",
    "senior" = "Experiencia",
    "genderHombre" = "Género: hombre",
    "partyDem." = "Afil.: dem.",
    "partyInd." = "Afil.: ind."
  ),
  stars = TRUE,
  vcov = "robust"
)
```

Si, en cambio, tenemos datos en panel (muchas unidades and varios periodos de tiempo), podemos usar errores estándar robustos por *cluster* o grupo. Miremos rápidamente con unos datos simulados en los cuales observamos 500 empresas (`firm`) a lo largo de diez años `year`, midiendo unas variables `x` y `y`. Como creemos que los residuos/errores pueden estar correlacionados entre empresas y años, incluimos una corrección con base en las variables relevantes:

```{r tab-cluster}
data("PetersenCL")
mod_firmas <- lm(x ~ y, data = PetersenCL)
modelsummary(
  mod_firmas,
  # esta formula indica que variables vamos a usar para los clusters
  vcov = ~ firm + year
)
```

Por último, podemos exportar una tabla a un documento Word:

```{r tab-guardar, eval = FALSE}
modelsummary(
  modelo_ingreso,
  output = "output/tabla_modelo_ingreso.docx"
)
```

### Gráficas

Visualizar los resultados de modelos, especialmente las predicciones y la incertidumbre de estos, puede ayudar mucho a entenderlos mejor y comunicar nuestros resultados. Vamos a usar la función `ggpredict` de `ggeffects`. También podríamos usar `sjPlot` o hacerlo completamente "a mano".

<!-- Primero, veamos cómo visualizar los coeficientes y su significancia. Si aplicamos `plot_model()` al objeto que contiene el modelo, producimos una gráfica que nos muestra los coeficientes de cada variable y sus intervalos de confianza (al 95%). Como `plot_model()` usa `ggplot2`, podemos modificar y agergar elementos usando la misma gramática de las gráficas.^[También se pueden cambiar casi todos los elementos dentro de `plot_model()`.] Por ejemplo, agregamos una línea de referencia para mostrar si los intervalos cubren 0: -->

<!-- ```{r plot-coef} -->
<!-- modelo_ingreso %>% -->
<!--   plot_model() + -->
<!--   scale_x_discrete(labels = c("Género: hombre", "Experiencia", "Educación")) +  # los ejes están trocados -->
<!--   geom_hline(yintercept = 0, linetype = "dashed") + -->
<!--   labs(title = "Resultados del modelo: coeficientes", -->
<!--        x = "Coeficientes", -->
<!--        y = "Estimados") -->
<!-- ``` -->

<!-- Así mismo, podemos comparar distintos modelos gráficamente con `plot_models()`: -->

<!-- ```{r plot-coef2} -->
<!-- plot_models( -->
<!--   modelo_ingreso, modelo_log, modelo_inter, -->
<!--   axis.labels = c("Educación", "Educación * hombre", "Hombre", "Experiencia",  -->
<!--                   "Experiencia (log)"), -->
<!--   legend.title = "Modelo", -->
<!--   m.labels = c("Completo", "Log", "Interacción") -->
<!-- ) -->
<!-- ``` -->

Como mencionamos arriba, podemos usar `ggpredict()` para ver los "efectos" de distintas variables, entendidos como el cambio en la variable dependiente para distintos valores de las independientes. Para esto, especificamos las variables independientes que nos interesan con `terms =`. Esta función mantiene constantes los factores (variables categóricas) en su categoría de referencia y las variables numéricas según su media. 

Le pasamos el modelo a la función ``ggpredict()`y luego pasamos los resultados de esta a `plot()`. A continuación, vemos el valor esperado (predicción) de la variable dependiente (ingreso anual) en todo el rango de la variable educación, controlando por género (categoría de referencia: mujer), experiencia (constante en la media) y afiliación partidista (republicano). Como `ggeffects` funciona con `ggplot2`, podemos modificar y agregar utilizando los trucos que ya conocemos:

```{r plot-eff}
modelo_ingreso %>%
  ggpredict(terms = "edu") %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación",
       subtitle = "Mujeres, controlando por experiencia laboral",
       x = "Educación (años)", y = "Ingreso (USD)")
```

Adicionalmente, podemos especificar dos variables independientes para visualizar aún más variación. Vemos cómo la "línea base" de ingreso (el intercepto) es distinto para hombres y mujeres. Así, vemos más claramente la diferencia en ingreso para hombres y mujeres, por ejemplo. Además, esto evidencia que incluir una variable categórica esencialmente crea un intercepto distinto para cada grupo, sin cambiar la pendiente (el coeficiente): 

```{r plot-eff2}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "gender")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia", 
       x = "Educación (años)", y = "Ingreso (USD)", color = "Género")
```

Si las dos variables son numéricas, `ggpredict()` automáticamente escoge tres valores del rango de la segunda variable que le pasamos (incluyendo la media) y grafica tres líneas:

```{r plot-eff3}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "senior")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Educación (años)", y = "Ingreso (USD)", color = "Experiencia")
```

Podemos espeficiar cuántas líneas queremos ver y los valores que debe tomar la segunda variable independiente:

```{r plot-ef3}
modelo_ingreso %>%
  ggpredict(terms = c("edu", "senior [5, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Educación (años)", y = "Ingreso (USD)", color = "Experiencia")
```

Finalmente, podemos cambiar el orden de las variables en `terms = ` si nos interesa resaltar el efecto de una variable más que la otra:

```{r plot-ef5}
modelo_ingreso %>%
  ggpredict(terms = c("gender", "edu [5, 15, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de género y educación",
       x = "Género", y = "Ingreso (USD)", color = "Educación")
```

```{r plot-ef6}
modelo_ingreso %>%
  ggpredict(terms = c("senior", "edu [5, 25]")) %>%
  plot() +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Valores esperados de ingreso para distintos valores de educación y experiencia",
       x = "Experiencia (años, log)", y = "Ingreso (USD)", color = "Educación")
```

## Conclusiones

Utilidad de los modelos de regresión lineal:

- Predicción: si cumplimos con los supuestos básicos, podemos hacer predicciones puntuales precisas basados en la asociación entre variables.
- Explicación: si cumplimos con más supuestos (evitamos problemas de endogeneidad y de sesgo por variables omitidas), podemos hacer inferencias sobre relaciones de tipo causal.

Otras posibilidades:

- Variables dependientes categóricas: regresión logística y modelos por máxima verosimilitud (MLE o *maximum likelihood estimation*).
- Experimentos: podemos usar OLS para encontrar la diferencia de medias.
- Data Science: regresión como modelo predictivo que se puede "entrenar".

Utilizar R para el análisis estadístico:

- Curva de aprendizaje, pero grandes posibilidades.
- Practicar, practicar y practicar.
- Google es nuestro amigo.

## Taller: regresión lineal

Encontrar una base de datos relevante para el proyecto de investigación. Limpiar y ordenar los datos. El código de esta entrega **no** debe incluir todo el procedimiento para cargar la base de datos original y el proceso de limpieza y organización de la misma.

### Estimación

Estimar dos modelos de regresión lineal **múltiple** usando la función `lm()` de R. Los modelos estimados deben cumplir con los siguientes requisitos (1.0 punto):

- La variable dependiente en ambos debe ser la variable dependiente del proyecto de investigación. *Idealmente* esta debe ser una variable de tipo continuo, pero excepcionalmente puede ser de tipo ordinal (si tiene $\geq 7$ categorías ordenadas) o binaria/dummy (entendiendo que la regresión lineal no es el mejor modelo para este tipo de variables). En estos dos casos, deben asegurarse que las variables estén codificadas (temporalmente) como tipo numérico o `dbl` en R. 
- Las variables independientes deben ser a) factores potencialmente explicativos de la variable dependiente y b) variables de control. La selección de variables debe estar basada en su revisión de literatura, teoría e hipótesis.
- Debe haber por lo menos una variable independiente numérica en uno de los dos modelos. Si no tienen una, deben buscar más datos.
- Debe haber por lo menos una variable independiente categórica binaria en uno de los dos modelos. Si no tienen una, deben crearla a partir de otra.
- Dado el punto 5 de este taller, es recomendable que por lo menos uno de los modelos tenga *ambos* tipos de variables.
- *Sugerencia*: Si la literatura sugiere un debate entre dos enfoques teóricos, pueden estimar un modelo que represente cada enfoque (por ejemplo, un modelo solo con variables institucionales y otro solo con variables sociológicas), seguido de un modelo con todas las variables. 
- O pueden construir el modelo completo de manera progresiva: un modelo con solo variables independientes de interés teórico, seguido de uno que añade variables de control.

### Comunicación de resultados

Presentar los resultados de los modelos estimados en una sola tabla de regresión, usando las funciones de la librería `modelsummary` (o alguna similar) para producirla, teniendo en cuenta las siguientes indicaciones (1.0 punto). 

- La tabla debe tener título y los coeficientes deben estar nombrados apropiadamente (o sea, con algo distinto al código de la variable en la base de datos).
- Usar errores estándar robustos o robustos *por cluster*, dependiendo del tipo de datos (corte transversal o panel).
- *Nota:* Pueden ejecutar `?modelsummary` en la consola para explorar todas las opciones de esta funciones y cómo utilizarlas.

### Diagnóstico

Evaluar si los modelos estimados cumplen el supuesto de homoesquedasticidad ($\text{Var}(e_i) = \sigma^2$) y si hay observaciones con un grado alto de influencia. Esta evaluación puede hacerse de manera visual *o* gráfica o usando tests estadísticos. Deben incluir un breve párrafo en el que consignen sus apreciaciones al respecto (1.0 punto).
  
### Interpretación

Interpretar sustantivamente *todos* los coeficientes de los modelos incluídos en la tabla y la incertidumbre en torno a estos estimados (pueden usar *p-values* o intervalos de confianza). *No* es necesario interpretar el coeficiente del intercepto. Cuando decimos "sustantivamente" nos referimos a que las interpretaciones deben hacer especial énfasis en la forma en que los resultados de los modelos se conectan con la evaluación de las hipótesis propuestas en el proyecto (1.0 punto).

### Interpretación gráfica

Usando uno de los dos modelos de regresión estimados en el primer punto, construir una gráfica con `ggpredict()` donde presenten valores esperados de la variable dependiente para el rango de valores de una variable independiente numérica y para todas las categorías de una variable categórica binaria (manteniendo constantes las demás variables). En otras palabras, la gráfica debe mostrar los valores esperados de la variable dependiente (con intervalos de confianza) para dos o más grupos (definidos por la variable categórica) a lo largo del rango de una variable independiente numérica (1.0 punto).

## Ejercicios adicionales

### Votos por Trump

Digamos que nos interesa estudiar los resultados de las elecciones presidenciales de 2016 en Estados Unidos. Para este proyecto, utilizamos una base de datos que incluye información sobre un conjunto de variables sociales y políticas a nivel de condado (municipio). Para esto, vamos a cargar la librería `socviz` que cuenta con datos sociopolíticos de Estados Unidos:

```{r}
library(socviz) # datos y herramientas para visualizar datos sociales
```

Específicamente, nos interesa la base de datos `county_data`, la cual limpiamos y reducimos:

```{r}
# seleccionamos y transformamos datos
county_data_sub <- county_data %>% 
  as_tibble() %>%
  select(name, state, census_region, black, white, hh_income,
         per_dem_2016, per_gop_2016, partywinner12) %>%
  mutate(partywinner12 = fct_explicit_na(partywinner12),
         black = black/100, 
         white = white/100, 
         hh_income = log(hh_income))

sample_n(county_data_sub, 10)
```

Veamos el resumen de algunas de estas variables. La tabla a continuación (construida con `datasummary()` de `modelsummary`) muestra la media y desviación estándar (variables numéricas) y el número de observaciones por grupo (variables categóricas):

```{r}
# estadisticas resumen
datasummary(
  # variables numericas
  (`Prop. afro` = black) +
    (`Prop. blanco` = white) +
    (`Ingreso promedio (log.)` = hh_income) +
    (`Prop. voto Dem.` = per_dem_2016) +
    (`Prop. voto Rep.` = per_gop_2016) ~ census_region *
    # funciones
    ((`Media` = Mean) + (`Desv. est.` = SD)),
  data = county_data_sub
)
```

Podemos mirar las frecuencias de las variables categóricas:

```{r}
datasummary(
  # variable categorica
  (`Partido 2012` = partywinner12) ~ 
    # funcion
    N,
  data = county_data_sub
)
```

También una tabla cruzada con dos variables categóricas y porcentajes además de frecuencias usando el operador `*`:

```{r}
datasummary(
  # variables categoricas
  (`Región` = census_region) * 
    (`Partido 2012` = partywinner12) + 1 ~ 
    # funciones
    N + Percent(),
  data = county_data_sub
)
```

O combinar variables numéricas y categóricas en una tabla de estadísticas descriptivas por grupos, usando nuevamente `*`:

```{r}
datasummary(
  # variables numericas
  (`Prop. afro` = black) +
    (`Prop. blanco` = white) +
    (`Ingreso promedio (log.)` = hh_income) +
    (`Prop. voto Dem.` = per_dem_2016) +
    (`Prop. voto Rep.` = per_gop_2016) ~ 
    # variables categoricas
    census_region *
    # funciones
    ((`Media` = Mean) + (`Desv. est.` = SD)),
  data = county_data_sub
)
```

Queremos entender la variación en la proporción de votos obtenidos por el candidato Demócrata (Donald Trump) en la elección de 2016 en cada condado. A continuación, vemos la distribución de esta variable:

```{r}
county_data %>%
  ggplot(aes(per_gop_2016)) +
  geom_histogram() +
  labs(x = "Prop. voto Rep.", y = "Número de condados")
```

Nuestra teoría sugiere que esperamos ver una mayor proporción de votos por Trump en condados con una mayor proporción de individuos que se identifican como blancos. La siguiente gráfica de dispersión explora esta relación:

```{r}
county_data %>%
  ggplot(aes(white, per_gop_2016)) +
  geom_point(alpha = 0.5) +
  labs(x = "Prop. blanco", y = "Prop. voto Rep.")
```

Para evaluar esta hipótesis, estimamos un modelo de regresión lineal múltiple. La variable dependiente es la proporción de votos por el candidato Republicano en cada condado. La variable independiente de interés es la proporción de habitantes blancos en cada condado. Además, incluimos varias variables de control: el logaritmo natural del ingreso promedio (en USD) de los hogares en cada condado, una *dummy* que indica el partido ganador en 2012 por condado y la región donde se ubica el condado. La ecuación que estimamos es:

$$ \hat{\text{Prop. voto Rep.}} = \alpha + \hat{\beta}_1 \times \text{Prop. blanco}_i + \hat{\beta}_2 \times \text{log(Ingreso)}_i + \hat{\beta}_3 \times \text{Partido 2012}_i + \hat{\beta}_4 \times \text{Región}_i + \hat{u}_i $$

La siguiente tabla muestra los resultados del modelo. Incluye el coeficiente ($\beta$) estimado para cada variable y el intercepto, así como intervalos de confianza del 95% entre paréntesis y la significancia estadística de cada coeficiente, además de unas estadísticas resumen del modelo (como el $\text{R}^2$).

```{r}
modelo_rep <- lm(
  per_gop_2016 ~ white + hh_income + partywinner12 + census_region,
  data = county_data
)

coefs <- c(
    "(Intercept)" = "Intercepto", 
    "white" = "Prop. Blanco", 
    "hh_income" = "Ingreso prom. (log.)",
    "partywinner12Republican" = "Partido 2012: Rep.", 
    "census_regionNortheast" = "Región: Nordeste", 
    "census_regionSouth" = "Región: Sur",
    "census_regionWest" = "Región: Oeste"
)

modelsummary(
  list(`Modelo 1` = modelo_rep), 
  stars = TRUE,
  coef_map = coefs
)
```

#### Interpretación

Para cada coeficiente, interpretar:

- Dirección.
- Magnitud.
- Significancia estadística.

#### Predicciones

Con la ecuación y los resultados, podemos hacer predicciones sobre el valor esperado de $Y$ (proporción de votos por Trump) para distintos valores de las variables independientes. ¿Cuál es el valor esperado de $Y$ para los siguientes perfiles de condados?

- 80% blanco, en la región Sur, en un condado en el que los Republicanos ganaron en 2012- las demás variables en sus medias (ver la tabla de estadísticas descriptivas).
- 20% blanco, en el Nordeste, en un condado en el que los Demócratas ganaron en 2012- nuevamente, establecemos las demás variables en sus medias.

### Duración de gobiernos democráticos

Algunos politólogos argumentan que la variación en la duración de gobiernos democráticos en el periodo posguerra se explica por características de los gobiernos de turnos: el nivel de apoyo parlamentario de los partidos en el gobierno, el número de partidos en el mismo y la disciplina que las organizaciones partidistas pueden imponer a sus miembros. En otras palabras, tenemos el siguiente modelo teórico de la duración de un gobierno:

$$ \text{Duración gob.} = f(\text{Apoyo parl. gob.} + \text{Núm. partidos gob.} + \text{Disciplina part.} + \text{Comp. estocástico}) $$

#### Datos

Como estudiantes de Ciencia Política, buscamos datos al respecto para evaluar este argumento y nos encontramos con una base de datos recopilada por Rob Franzese hace unos años a partir del *Political Data Handbook of OECD Countries* con datos para países de la OCDE. La base de datos se encuentra en el archivo `OECD_country_data.xls` [aquí](https://github.com/josefortou/lab-book/tree/master/data). 

- Cargue los datos en R usando las funciones y librerías apropiadas para un archivo `.xls`.

Afortunadamente, el archivo también incluye un diccionario de variables (en inglés) en la segunda pestaña del archivo. Pista: `read_excel()` tiene un argumento `sheet = `.

- Identifique las variables relevantes para estimar el modelo especificado arriba y asegúrese que estén en el formato adecuado.

#### Regresión

La información contenida en esta base de datos nos permite decir algo sobre el argumento teórico sugerido al inicio.

- Utilizando estos datos, estime un modelo de regresión lineal sencillo (sin utilizar variables transformadas) que evalúe el argumento, presente los resultados en una tabla de regresión e interpretelos de forma sustancial (en términos del argumento).

#### Críticas

Frente a estos resultados, una colega sugiere que lo que está sucediendo es un poco distinto. Ella dice: "la disciplina partidista no tiene un efecto independiente sobre la duración de los gobiernos. Más bien, condiciona el efecto del apoyo legislativo al gobierno y del número de partidos en la coalición sobre la duración".

- Especifique un modelo que tome en cuenta esta crítica, estímelo via OLS e interprete los efectos de las variables independientes sobre la dependiente:

#### Extra: comparar modelos

¿Cómo adjudicamos entre estos modelos? Otra colega sugiere que una prueba *F*, realizada por medio de un análisis de varianza (ANOVA) de los dos modelos, podría decirnos algo sobre cuál es mejor.

- Realice una prueba *F* y concluya cuál modelo prefiere en términos estadísticos y teóricos:

#### Comunicar resultados

Es útil visualizar los resultados de modelos estadísticos como estos. Para el siguiente punto, seleccione el modelo que crea que se ajusta mejor a los datos y representa mejor la teoría.

- Construya una gráfica que muestre el efecto del número de partidos en la coalición de gobierno y del apoyo legislativo al gobierno sobre la duración de los mismos cuando la disciplina partidista es alta y cuando es baja. Incluya intervalos de confianza en la gráfica.

### Opinión pública sobre el presidente

Supongamos que queremos entender las percepciones y evaluaciones de individuos sobre los presidentes, particularmente los de USA. Por inspiración de las diosas politológicas, sabemos que el modelo "real" es:

$$ \text{Eval. presid.} = \alpha + \beta_1\text{Afroamericano} + \beta_2\text{ID partidista} + \beta_3\text{Ideología} + \beta_4\text{Eval. idiosincrática} + \beta_5\text{Eval. sociotrópica} + \epsilon $$

Esta es una versión de un modelo clásico en Ciencia Política, el cual considera que las evaluaciones individuales de los políticos son una función de posiciones ideológicos y percepciones de la economía. Queremos evaluar este modelo. 

#### Datos 

Nuevamente, buscamos datos y encontramos la encuesta del *American National Election Study* (ANES) para 2004, en la cual le preguntaron a los encuestados por su percepción del entonces presidente George W. Bush. Los datos se encuentran en el archivo `nes_2004_data.csv` [aquí](https://github.com/josefortou/lab-book/tree/master/data). Las principales variables son:

| Nombre de variable | Definición |
|--------------------|-----------------------------------------------------------------------------------------|
| thermom | Sentimientos por Bush (1:100; 0=mal, 100=bien) |
| race | Raza/etnicidad (10, 12, 13, 14 y 15=afroamericano) |
| partyid | Identificación partidista (0=muy Demócrata, 3=independiente, 6=muy Republicano, 7=otro) |
| libcon | ¿Liberal o conservador? (1=liberal, 3=moderado, 5=conservador, 7=no responde) |
| betteroff | Situación económica familiar el último año (1-5; 1=peor, 3=igual, 5=mejor) |
| economy | Situación económica del país el último año (1-5; 1=peor, 3=igual, 5=mejor) |
| edulevel | Nivel educativo máximo (0-7; 0=sin bachillerato, 7=posgrado) |

- Cargue el archivo de datos y realice los procedimientos necesarios para asegurarse que las variables estén debidamente codificadas.

#### Modelos

Aprovechemos que sabemos que el modelo de arriba es el "real".

- Estime el modelo "real", interprete los coeficientes y diga algo sobre la interpretación *causal* de los mismos.

- Estime el siguiente modelo -llamémoslo "modelo 2"-, que tiene una variable menos y donde los coeficientes son estimados ($\hat{\beta}$ en vez de $\beta$):

$$ \text{Eval. presid.} = \alpha + \hat{\beta_1}\text{Afroamericano} + \hat{\beta_2}\text{ID partidista} + \hat{\beta_3}\text{Eval. idiosincrática} + \hat{\beta_4}\text{Eval. sociotrópica} + \epsilon $$

- Estime este otro modelo -llamémoslo algo creativo, como "modelo 3"-, que tiene una variable más y donde los coeficientes son estimados ($\hat{\beta}$ en vez de $\beta$):

$$ \text{Eval. presid.} = \alpha + \hat{\beta_1}\text{Afroamericano} + \hat{\beta_2}\text{ID partidista} + \hat{\beta_3}\text{Ideología} + \hat{\beta_4}\text{Eval. idiosincrática} + \hat{\beta_5}\text{Eval. sociotrópica} + \hat{\beta_6}{Educación} + \epsilon $$

#### Extra: presentar resultados

- Presente los resultados de los tres modelos en una tabla y saque conclusiones sobre el *sesgo* de los coeficientes en el modelo 2 y la *eficiencia* del modelo 3.

- Presente gráficamente y discuta los efectos de todas las variables independientes en uno de los modelos.



